{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return y - y_hat\n",
    "    return np.mean(1/2*np.sum(np.power(y - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "        \n",
    "    return W, B\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-1].shape[0]\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)):\n",
    "        Z.insert(i, W[i] @ A[i] + B[i])\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #comuns are the samples now\n",
    "    return Z, A, Y\n",
    "\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = ( -(T - Y_hat ) * logistic_sigmoid(Z[-1], derivative=1))\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "#         dJ_dW.insert(i+1, error[i+1] @ A[i+1].T)\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "#         error.insert(i, error_dummy)\n",
    "        error[i] = error_dummy\n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    \n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#online (sample by sample) training\n",
    "# all samples (X), 4 x 2, are fed\n",
    "\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#train package is a list with [W1, b1, W2, b2, X]\n",
    "# n: 0-3 selection of logical inputs; e.g. 0 == [0, 0]; 3 == [1,1]\n",
    "def predict(train_pkg):\n",
    "    Z2, Z3, Y = forward_prop(train_pkg[0], train_pkg[1],\\\n",
    "                             train_pkg[2], train_pkg[3], \\\n",
    "                             train_pkg[4])\n",
    "    del Z2, Z3\n",
    "    \n",
    "    return np.round(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, no_hidden_units=2,\\\n",
    "                    iterations=500, rho=.01, print_cost=0):\n",
    "    train_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # NO_UNITS_L1 = 6  yields max matches with rho = 1 and epochs = 500\n",
    "#         train_gates[i] : [W1, b1, W2, b2, X, Y, idx_done, epochs, converged, rho]\n",
    "        train_gates[i] = train(X, t[i], NO_UNITS_L1=no_hidden_units,\\\n",
    "                               epochs=iterations, learning_rate=rho,\\\n",
    "                               show_cost=print_cost)\n",
    "    return train_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_logic_gate >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_logic_gate(train_pkg, T):\n",
    "\n",
    "    Y = predict(train_pkg)\n",
    "    prediction_match = np.array_equal(Y, T)\n",
    "#         train_pkg : [W1, b1, W2, b2, X, Y, idx_done, epochs, converged, rho]\n",
    "    # indeces used, especially:\n",
    "    #                                 8. converged\n",
    "    #                                 6. idx_done, \n",
    "    #                                 7. epochs\n",
    "    #                                 9. rho\n",
    "    match_pkg = [train_pkg[8], train_pkg[6],\\\n",
    "                  train_pkg[7], train_pkg[9],\\\n",
    "                  prediction_match, Y]\n",
    "        \n",
    "    return match_pkg\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_all_gate_outputs(train_pkg_all_gates, t):\n",
    "    matches = {}\n",
    "\n",
    "    for i in t:\n",
    "        matches[i] = match_logic_gate(train_pkg_all_gates[i], t[i])\n",
    "        \n",
    "    return matches\n",
    "\n",
    "def print_match(match):\n",
    "    print(i + \" converged: \" + str(match[0]))\n",
    "    print(\"===========================================\")\n",
    "    print(\"  iter. to converge: \" + str(match[1]))\n",
    "    print(\"  iter. max: \" + str(match[2]))\n",
    "    if match[4]:\n",
    "        print(\"  ==== CORRECT prediction ==== \")\n",
    "    else:\n",
    "        print(\"  ==== INCORRECT prediction ==== \")\n",
    "    print(\"  predicted y (y_hat): \")\n",
    "    print(match[5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "def train(X, T, hidden_layers=[2], epochs=500, rho=.1, show_cost=0):\n",
    "    no_of_features = X.shape[1]\n",
    "    no_outputs = T.shape[1]\n",
    "    W, B = init_weights_biases(no_of_features, no_outputs, hidden_layers)\n",
    "\n",
    "    j = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    for i in range(epochs):\n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW = backprop(W, Z, A, Y_hat, T.T)\n",
    "\n",
    "        #grad descent\n",
    "        for i in range(len(W)):\n",
    "            W[i] = W[i] - rho*dJ_dW[i]\n",
    "\n",
    "        if show_cost:\n",
    "            plt.scatter(i, cost(T,Y_hat.T))\n",
    "#             print(\"cost: \" + str(cost(T,Y_hat.T)))\n",
    "        \n",
    "        Y = np.round(Y_hat).T\n",
    "        y_and_T_match = np.array_equal(Y, T)        \n",
    "\n",
    "        if y_and_T_match: #converged\n",
    "            j += 1 \n",
    "            if j == 3:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            if j > 100: #makes the prediction more robust \n",
    "                # ( probability considered 1 == .60 or greater )\n",
    "                converged = True\n",
    "                break\n",
    "    \n",
    "    return W, B, Y, X, idx_done, epochs, converged, rho\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#train package is a list with [W1, b1, W2, b2, X]\n",
    "# n: 0-3 selection of logical inputs; e.g. 0 == [0, 0]; 3 == [1,1]\n",
    "def predict(train_pkg):\n",
    "    Z2, Z3, Y = forward_prop(train_pkg[0], train_pkg[1],\\\n",
    "                             train_pkg[2], train_pkg[3], \\\n",
    "                             train_pkg[4])\n",
    "    del Z2, Z3\n",
    "    \n",
    "    return np.round(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'fisheriris.xlsx')\n",
    "names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "idx = [50,100,150]\n",
    "\n",
    "# first four columns are \n",
    "# X = np.zeros((150,4))\n",
    "X = np.array(df.values[:,0:4], dtype=np.float32)\n",
    "T = np.zeros((150,3))\n",
    "T[0:50,0] = 1 # setosa\n",
    "T[50:100,1] = 1 #versicolor\n",
    "T[100:150,2] = 1 #virginica\n",
    "idx = np.random.randint(0,high=149,size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "# Z, A, Y_hat = forward_prop(W, B, X[0:50,0:4].T)\n",
    "# dJ_dW = backprop(W, Z, A, Y_hat, T[0:50,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(0,high=149,size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pkg = train(X,T)\n",
    "train_pkg = train(X, T, hidden_layers=[2,2,3,2,2], epochs=30_000, rho=.01, show_cost=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
