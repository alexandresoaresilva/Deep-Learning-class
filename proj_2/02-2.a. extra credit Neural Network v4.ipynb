{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "# from numba import vectorize\n",
    "# from numba import cuda\n",
    "# from numba import autojit\n",
    "# import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cuda.jit\n",
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "# @cuda.jit(device=True)\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend\n",
    "\n",
    "slight modification from previous network here. It uses a non normalized output (does not become a probability), so now both outputs can be either 0's or 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "# @cuda.jit(device=True)\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "# @autojit\n",
    "# @cuda.jit\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        Z.insert(i, W[i] @ A[i] + B[i])\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "#     Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "#     #scaling to making the pair a probability\n",
    "#     Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #columns are the samples now\n",
    "    Y = A[i+1]\n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "# @cuda.jit(device=True)\n",
    "\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1)\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "#         dJ_dW.insert(i+1, error[i+1] @ A[i+1].T)\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "#         error.insert(i, error_dummy)\n",
    "        error[i] = error_dummy\n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    \n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "# @cuda.jit(device=True)\n",
    "# @autojit\n",
    "def train(X, T, hidden_layers=[2], epochs=500,\\\n",
    "          rho=.1, normalize_data=False, show_cost=0, seed=1, w_transfer=[]):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features,\\\n",
    "                                      no_outputs, hidden_layers, seed=1)\n",
    "    \n",
    "    if w_transfer:\n",
    "        W=w_transfer\n",
    "        \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))    \n",
    "\n",
    "    N = int(np.round(epochs/20))\n",
    "    i_for_show_cost = np.round(np.linspace(0,epochs,N))\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "        \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    #loop variables, along with other variables and lists (cost, accuracy)\n",
    "    # that are returned by the function\n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    iter_passed = []\n",
    "    match = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    k_cost = 0\n",
    "    acc_count = 0\n",
    "    for i in range(epochs):\n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW = backprop(W, Z, A, Y_hat, T.T)\n",
    "        \n",
    "        sign = 1\n",
    "    \n",
    "        #grad descent\n",
    "        for a in range(len(W)):\n",
    "            W[a] = W[a] - sign*rho*dJ_dW[a]\n",
    "        \n",
    "        if match <=5:\n",
    "            idx_done = i + 1\n",
    "        \n",
    "        if is_equal_in_boolean_terms(Y_hat.T, T):\n",
    "            match += 1            \n",
    "                \n",
    "            if match > 6 and match < 7:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            \n",
    "        if match > 20: #seeing the match 20 times \n",
    "            converged = True #makes the predictions more confident\n",
    "            break\n",
    "        \n",
    "        \n",
    "        cost_final.insert(i,cost_MSE(T, Y_hat.T))\n",
    "        accuracy.insert(i,calc_accuracy(T, Y_hat.T))\n",
    "        iter_passed.insert(i,i)\n",
    "        \n",
    "        if accuracy[i] == 100:\n",
    "            acc_count +=1\n",
    "        \n",
    "#         if acc_count > 100_000:\n",
    "#             break\n",
    "            \n",
    "        if show_cost and i_for_show_cost[k_cost] == i:\n",
    "            print(\"   \" + str(i) + \" iteration, accuracy: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\"   cost: \" + str(cost_final[i]))\n",
    "            k_cost += 1        \n",
    "        \n",
    "    if converged:\n",
    "        print(\"   Converged in \" + str(idx_done) + \" iterations\")\n",
    "    else:\n",
    "        print(\"   Did not converge.\")\n",
    "    print()\n",
    "    if show_cost:\n",
    "        plt.scatter(iter_passed, cost_final, s=1, color=\"red\")\n",
    "        plt.title(\"iterations X cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        \n",
    "    return [W, B, Y_hat.T, X, cost_final, epochs,\\\n",
    "            idx_done, converged, rho, normalize_data, accuracy]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy]\n",
    "def predict(X, T, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "def is_equal_in_boolean_terms(Y_hat, T):\n",
    "    Y_hat = np.round(Y_hat)\n",
    "    \n",
    "#     if len(Y_hat.shape) > 1:\n",
    "#         if Y_hat.shape[1] > 1:\n",
    "#             for i in range(len(Y_hat[:,0])):\n",
    "#                 if np.abs(Y_hat[i,0] - Y_hat[i,1]) < .25:\n",
    "#                     Y_hat[i,0], Y_hat[i,1] = 0, 0\n",
    "    \n",
    "    print(Y_hat)\n",
    "    return np.array_equal(Y_hat, T)\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN functions 2 (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, hidden_layers=[3], iterations=500, rho=.01, normalize_data=False, print_cost=0):\n",
    "    train_pkg_all_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # hidden_layers = list of number of units for each layer. Minimum 1 [1]\n",
    "                #train_gates[i] : [W, B, Y, X, cost_final, epochs, idx_done, \\\n",
    "                            #  converged, rho, normalize_data, accuracy]\n",
    "        train_pkg_all_gates[i] = train(X, t[i],\\\n",
    "                                       hidden_layers = hidden_layers, \\\n",
    "                               epochs=iterations, rho=rho, \\\n",
    "                                       normalize_data=normalize_data, \\\n",
    "                                       show_cost=print_cost, seed=1)\n",
    "\n",
    "    return train_pkg_all_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_logic_gate >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_logic_gate(X, T, train_pkg):\n",
    "    #y_hat is returned transposed already from predict\n",
    "    Y_hat, accuracy = predict(X, T, train_pkg)\n",
    "    Y_hat = np.round(Y_hat)\n",
    "#   train_pkg : [W, B, Y, X, cost_final, epochs, idx_done,\\\n",
    "#                converged, rho, normalize_data, accuracy]\n",
    "    # indeces used, especially:\n",
    "    #                                 7. converged\n",
    "    #                                 6. idx_done, \n",
    "    #                                 5. epochs\n",
    "    #                                 8. rho\n",
    "    match_pkg = [train_pkg[7], train_pkg[6],\\\n",
    "                  train_pkg[5], train_pkg[8],\\\n",
    "                  is_equal_in_boolean_terms(Y_hat, T), Y_hat]\n",
    "    return match_pkg\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_all_gate_outputs(X, T, train_pkg_all_gates):\n",
    "    matches = {}\n",
    "\n",
    "    for i in t:\n",
    "        matches[i] = match_logic_gate(X, T[i], train_pkg_all_gates[i])\n",
    "\n",
    "    return matches\n",
    "\n",
    "def print_match(match):\n",
    "    print(i + \" converged: \" + str(match[0]))\n",
    "    print(\"===========================================\")\n",
    "    if match[1]:\n",
    "        print(\"  iter. to converge: \" + str(match[1]))\n",
    "    else:\n",
    "        print(\"  reached max. iter\")\n",
    "    print(\"  iter. max: \" + str(match[2]))\n",
    "    if match[4]:\n",
    "        print(\"  ==== CORRECT prediction ==== \")\n",
    "    else:\n",
    "        print(\"  ==== INCORRECT prediction ==== \")\n",
    "    print(\"  predicted y (y_hat): \")\n",
    "    print(np.round(match[5]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset for extra credit - binary adder with carry\n",
    "\n",
    "same network is used, just with a different dataset. The network adjusts inputs and outpus automatically. Just a new target is needed.\n",
    "\n",
    "    Truth table for binary adder with Carry\n",
    "    A B | Sum | C_out\n",
    "    0 0 |   0 | 0\n",
    "    0 1 |   1 | 0\n",
    "    1 0 |   1 | 0\n",
    "    1 1 |   0 | 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = {\"ADDER\": np.array([[0, 0],\\\n",
    "                            [1, 0],\\\n",
    "                            [1, 0],\\\n",
    "                            [0, 1]], dtype=np.float32) }\n",
    "\n",
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN run\n",
    "\n",
    "The Neural Network can be run with train() and then feeding the train_package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 32 units\n",
      "   layer 3: 64 units\n",
      "   layer 4: 32 units\n",
      "   output layer (5): 2\n",
      "   learnable weights: 4224\n",
      "   max epochs: 10,000,000\n",
      "   learning rate(rho): 0.3\n",
      "[[1. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "   0 iteration, accuracy: 25.0%\n",
      "   cost: 0.9906016177295212\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 1.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.07815374393726127\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "   Converged in 17 iterations\n",
      "\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pkg_adder = train(X, T[\"ADDER\"], hidden_layers=[32,64,32], epochs=10_000_000,\\\n",
    "                    rho=.3, normalize_data=True, show_cost=1)#, w_transfer=W)   \n",
    "match_adder = match_logic_gate(X, T[\"ADDER\"], train_pkg_adder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches\n",
    "matches is a Python dictionary. For ex: \n",
    "\n",
    "    matches\n",
    "    returns a list:\n",
    "            matches[0] == boolean (converged or not, True or False)\n",
    "            matches[1] == idx_done, no. of iterations to converge\n",
    "            matches[2] == total iterations (epochs)\n",
    "            matches[3] == learning rate used(rho)\n",
    "            matches[4] == matches with target (comparing both F_out and C_out))\n",
    "            matches[5] == predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary adder with carry ====================================\n",
      "    converged: True\n",
      "    no. of iterations to converge: 17\n",
      "    total iterations set: 10000000\n",
      "    learning reate: 0.3\n",
      "    matches with target: True\n",
      "    prediction: \n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"binary adder with carry ====================================\")\n",
    "print(\"    converged: \" + str(match_adder[0]))\n",
    "print(\"    no. of iterations to converge: \" + str(match_adder[1]))\n",
    "print(\"    total iterations set: \" + str(match_adder[2]))\n",
    "print(\"    learning reate: \" + str(match_adder[3]))\n",
    "print(\"    matches with target: \" + str(match_adder[4]))\n",
    "print(\"    prediction: \")\n",
    "print(match_adder[5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
