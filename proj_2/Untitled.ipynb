{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "# A class that models the Neural Net with L-layers and\n",
    "# N neurons in each layer. It also contains the functions\n",
    "# for training, testing, and optimizing the Neural Network\n",
    "\n",
    "class DeepNN:\n",
    "\n",
    "    # Constructor to build the structure of the Neural Network\n",
    "    # It accepts the layers in the format of [2,3,1] -> 2 Neuron Input Layer,\n",
    "    # 3 Neuron Hidden Layer and 1 Neuron output layer\n",
    "    def __init__(self, layers):\n",
    "        ############################### Initialize the number of layers and neurons\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.hidden_layers = len(layers) - 2\n",
    "        self.input_neurons = layers[0]\n",
    "        self.output_neurons = layers[-1]\n",
    "\n",
    "        ########## Intialize parameters for Forward Propogation\n",
    "        # Initialize Weights\n",
    "        self.epsilon = 0.12  # Random Weight Initialization Factor\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-2):\n",
    "            self.weights.append(np.random.randn(layers[i]+1, layers[i+1]+1)*2*self.epsilon - self.epsilon)\n",
    "                        # We add a +1 to incorporate for weights from the +1 neuron for the bias\n",
    "        self.weights.append(np.random.randn(layers[-2]+1, layers[-1])*2*self.epsilon - self.epsilon)\n",
    "\n",
    "        self.a = [] # To keep track of activations\n",
    "        self.z = [] # To keep track of layer values\n",
    "        self.activations = ['sigmoid']*(self.num_layers-1) # Activations for each layer\n",
    "\n",
    "        ######### Intialize parameters for Backward Propogation\n",
    "        self.delta = []\n",
    "        self.gradient = []\n",
    "\n",
    "        # Initialize Scaling\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    ################################### Define Some Activation Functions and their derivatives ##################\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def reLU(self,x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def reLUPrime(self,x):\n",
    "        return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "    def tanh(self,z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def tanh_prime(self,x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "    def identity(self,x):\n",
    "        return x\n",
    "\n",
    "    def identity_prime(self,x):\n",
    "        return 1\n",
    "\n",
    "    ######################################### Cost Functions #############################################\n",
    "    # Least Squares\n",
    "    def least_squares_cost(self,t):\n",
    "        return 0.5*np.mean( (t-self.a[-1])**2 )\n",
    "\n",
    "    # Cross Entropy Log Loss Function\n",
    "    def log_loss(self,t):\n",
    "        return np.mean( np.nan_to_num( -1*t*np.log(self.a[-1]) - (1-t)*np.log(1-self.a[-1]) ) )\n",
    "\n",
    "    ######################################### Forward Feed ##############################################\n",
    "    def forwardFeed(self, X, activations):\n",
    "        self.activations = activations\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "\n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "#         print(a)\n",
    "        for i in range(self.num_layers-1):\n",
    "#             print(a[i])\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "\n",
    "    def backPropogate(self,y):\n",
    "\n",
    "        delta = []\n",
    "        gradient = []\n",
    "#         print('Weights:', self.weights)\n",
    "        weights_flipped = self.weights[::-1]\n",
    "        z_flipped = self.z[::-1]\n",
    "        activations_flipped = self.a[::-1]\n",
    "        activation_func_flipped = self.activations[::-1]\n",
    "        delta.append(activations_flipped[0] - y)\n",
    "#         print('Weights Flipped:', weights_flipped)\n",
    "        for i in range(0,self.num_layers-2):\n",
    "#                 print('delta: ',delta[i])\n",
    "#                 print('weights_flipped: ',weights_flipped[i])\n",
    "#                 print('z_flipped: ',z_flipped[i+1])\n",
    "                if(activation_func_flipped[i] == 'sigmoid'):\n",
    "#                     print('Sigmoid Prime')\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.sigmoidPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'reLU'):\n",
    "#                     print('reLU Prime')\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.reLUPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'tanh'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.tanh_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'identity'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.identity_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'softmax'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T))\n",
    "\n",
    "        delta = delta[::-1]\n",
    "\n",
    "        for i in range(len(delta)):\n",
    "            gradient.append( np.dot(self.a[i].T, delta[i]) )\n",
    "\n",
    "        self.delta = delta\n",
    "        self.gradient = gradient\n",
    "\n",
    "    def learn(self, epochs, learning_rate, X, y, activations, cost_func, metrics_at=10,\\\n",
    "              optimizer = '', batch_size=10, scaler_type='standard_scaler', split=False, test_size=0.25):\n",
    "        start = time.time()\n",
    "\n",
    "        if scaler_type == 'min_max_scaler':\n",
    "            self.scaler = preprocessing.MinMaxScaler()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "\n",
    "        if split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "\n",
    "        for i in range(epochs):\n",
    "                if optimizer == 'sgd':\n",
    "                    random_indics = np.random.randint(len(y),size=batch_size)\n",
    "                    X_sgd = X[random_indics]\n",
    "                    y_sgd = y[random_indics]\n",
    "                    self.forwardFeed(X_sgd, activations)\n",
    "                    self.backPropogate(y_sgd)\n",
    "                else:\n",
    "                    self.forwardFeed(X, activations)\n",
    "                    self.backPropogate(y)\n",
    "\n",
    "                for j in range(len(self.gradient)):\n",
    "                    self.weights[j] = self.weights[j] - learning_rate*self.gradient[j]\n",
    "\n",
    "                if(i%metrics_at == 0):\n",
    "                    self.forwardFeed(X, activations)\n",
    "                    print('Effective epoch: ', i/metrics_at + 1)\n",
    "                    if(cost_func == 'log_loss'):\n",
    "                        cost = self.log_loss(y)\n",
    "                        print('Accuracy: ', np.mean(np.round(self.think(X))==y) * 100, '%')\n",
    "                    elif(cost_func == 'least_squares'):\n",
    "                        cost = self.least_squares_cost(y)\n",
    "                    print('Cost: ', cost, '\\n')\n",
    "\n",
    "        if(cost_func == 'log_loss' and split):\n",
    "            print('Testing Accuracy: ', np.mean(np.round(self.think(X_test))==y_test) * 100, '%')\n",
    "\n",
    "        end = time.time()\n",
    "        print('Time Taken: ', end-start, ' seconds')\n",
    "        return self.weights\n",
    "\n",
    "    def think(self,X):\n",
    "\n",
    "        X = self.scaler.fit_transform(X)\n",
    "\n",
    "        activations = self.activations\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "\n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "#         print(a[0].shape)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective epoch:  1.0\n",
      "Cost:  0.11149529061126406 \n",
      "\n",
      "Effective epoch:  2.0\n",
      "Cost:  0.055929010466871557 \n",
      "\n",
      "Effective epoch:  3.0\n",
      "Cost:  0.03499969780547974 \n",
      "\n",
      "Effective epoch:  4.0\n",
      "Cost:  0.005854006661965788 \n",
      "\n",
      "Effective epoch:  5.0\n",
      "Cost:  0.010487682033842087 \n",
      "\n",
      "Effective epoch:  6.0\n",
      "Cost:  0.0059976240951241325 \n",
      "\n",
      "Effective epoch:  7.0\n",
      "Cost:  0.004954811124864652 \n",
      "\n",
      "Effective epoch:  8.0\n",
      "Cost:  0.005356476727540838 \n",
      "\n",
      "Effective epoch:  9.0\n",
      "Cost:  0.004293327989151456 \n",
      "\n",
      "Effective epoch:  10.0\n",
      "Cost:  0.010046308824456483 \n",
      "\n",
      "Time Taken:  0.3140702247619629  seconds\n",
      "[[9.97813624e-01 1.30735179e-03 8.78708582e-04]\n",
      " [9.97810757e-01 1.30868579e-03 8.79147191e-04]\n",
      " [9.97813075e-01 1.30760720e-03 8.78792749e-04]\n",
      " [9.97812181e-01 1.30802288e-03 8.78929534e-04]\n",
      " [9.97813744e-01 1.30729638e-03 8.78690312e-04]\n",
      " [9.97813760e-01 1.30728877e-03 8.78687803e-04]\n",
      " [9.97813447e-01 1.30743407e-03 8.78735707e-04]\n",
      " [9.97813410e-01 1.30745164e-03 8.78741499e-04]\n",
      " [9.97810259e-01 1.30891786e-03 8.79223223e-04]\n",
      " [9.97812393e-01 1.30792450e-03 8.78897185e-04]\n",
      " [9.97813754e-01 1.30729146e-03 8.78688691e-04]\n",
      " [9.97813410e-01 1.30745155e-03 8.78741468e-04]\n",
      " [9.97811866e-01 1.30816965e-03 8.78977777e-04]\n",
      " [9.97812979e-01 1.30765209e-03 8.78807531e-04]\n",
      " [9.97813866e-01 1.30723964e-03 8.78671596e-04]\n",
      " [9.97813882e-01 1.30723207e-03 8.78669101e-04]\n",
      " [9.97813818e-01 1.30726190e-03 8.78678940e-04]\n",
      " [9.97813511e-01 1.30740444e-03 8.78725942e-04]\n",
      " [9.97813692e-01 1.30732016e-03 8.78698155e-04]\n",
      " [9.97813799e-01 1.30727083e-03 8.78681884e-04]\n",
      " [9.97813004e-01 1.30764004e-03 8.78803571e-04]\n",
      " [9.97813670e-01 1.30733055e-03 8.78701579e-04]\n",
      " [9.97813829e-01 1.30725686e-03 8.78677277e-04]\n",
      " [9.97810429e-01 1.30883829e-03 8.79197187e-04]\n",
      " [9.97813146e-01 1.30757416e-03 8.78781865e-04]\n",
      " [9.97809318e-01 1.30935622e-03 8.79366712e-04]\n",
      " [9.97812759e-01 1.30775441e-03 8.78841223e-04]\n",
      " [9.97813558e-01 1.30738256e-03 8.78718729e-04]\n",
      " [9.97813406e-01 1.30745350e-03 8.78742114e-04]\n",
      " [9.97812627e-01 1.30781561e-03 8.78861358e-04]\n",
      " [9.97811596e-01 1.30829532e-03 8.79019056e-04]\n",
      " [9.97812522e-01 1.30786420e-03 8.78877363e-04]\n",
      " [9.97813881e-01 1.30723270e-03 8.78669308e-04]\n",
      " [9.97813882e-01 1.30723224e-03 8.78669156e-04]\n",
      " [9.97811750e-01 1.30822346e-03 8.78995457e-04]\n",
      " [9.97813008e-01 1.30763845e-03 8.78803044e-04]\n",
      " [9.97813573e-01 1.30737556e-03 8.78716423e-04]\n",
      " [9.97813794e-01 1.30727281e-03 8.78682538e-04]\n",
      " [9.97812030e-01 1.30809336e-03 8.78952697e-04]\n",
      " [9.97813372e-01 1.30746927e-03 8.78747309e-04]\n",
      " [9.97813587e-01 1.30736942e-03 8.78714396e-04]\n",
      " [9.97643155e-01 1.38855851e-03 9.02894497e-04]\n",
      " [9.97813230e-01 1.30753510e-03 8.78768999e-04]\n",
      " [9.97812523e-01 1.30786410e-03 8.78877320e-04]\n",
      " [9.97813656e-01 1.30733716e-03 8.78703761e-04]\n",
      " [9.97809740e-01 1.30915930e-03 8.79302287e-04]\n",
      " [9.97813816e-01 1.30726283e-03 8.78679247e-04]\n",
      " [9.97813012e-01 1.30763645e-03 8.78802382e-04]\n",
      " [9.97813764e-01 1.30728673e-03 8.78687128e-04]\n",
      " [9.97813190e-01 1.30755365e-03 8.78775113e-04]\n",
      " [1.40567387e-03 9.87122928e-01 1.01902091e-02]\n",
      " [1.41812777e-03 9.87023845e-01 1.02314075e-02]\n",
      " [1.38660210e-03 9.87180593e-01 1.01925727e-02]\n",
      " [1.38498351e-03 9.87272082e-01 1.01313596e-02]\n",
      " [1.38252068e-03 9.87218167e-01 1.01751155e-02]\n",
      " [1.39545310e-03 9.87089272e-01 1.02371053e-02]\n",
      " [1.40585339e-03 9.86833112e-01 1.03932065e-02]\n",
      " [1.50551942e-03 9.86363621e-01 1.04806130e-02]\n",
      " [1.39312491e-03 9.87216652e-01 1.01523997e-02]\n",
      " [1.41433287e-03 9.87053002e-01 1.02196922e-02]\n",
      " [1.38908580e-03 9.87248992e-01 1.01385714e-02]\n",
      " [1.41309814e-03 9.87062548e-01 1.02157806e-02]\n",
      " [1.38552480e-03 9.87275504e-01 1.01277234e-02]\n",
      " [1.37690671e-03 9.86957586e-01 1.03725589e-02]\n",
      " [1.53141391e-03 9.86160527e-01 1.05544798e-02]\n",
      " [1.40943382e-03 9.87095008e-01 1.02013116e-02]\n",
      " [1.36091579e-03 9.86211426e-01 1.09377061e-02]\n",
      " [1.43480266e-03 9.86903181e-01 1.02773040e-02]\n",
      " [1.27475354e-03 9.85452267e-01 1.16983677e-02]\n",
      " [1.40177447e-03 9.87153547e-01 1.01774914e-02]\n",
      " [9.54587939e-06 2.87790655e-02 9.72266320e-01]\n",
      " [1.40623814e-03 9.87119549e-01 1.01913204e-02]\n",
      " [7.46204342e-05 4.53407097e-01 5.36558386e-01]\n",
      " [1.39314658e-03 9.87180482e-01 1.01779227e-02]\n",
      " [1.40228095e-03 9.87149244e-01 1.01793633e-02]\n",
      " [1.39956105e-03 9.87169498e-01 1.01712222e-02]\n",
      " [1.38336127e-03 9.87244801e-01 1.01543366e-02]\n",
      " [3.05408751e-04 8.87911533e-01 1.02005466e-01]\n",
      " [1.38390802e-03 9.87071929e-01 1.02755966e-02]\n",
      " [1.46677799e-03 9.86658555e-01 1.03717224e-02]\n",
      " [1.39685236e-03 9.87190617e-01 1.01624345e-02]\n",
      " [1.40725883e-03 9.87112298e-01 1.01941244e-02]\n",
      " [1.41446057e-03 9.87057581e-01 1.02161649e-02]\n",
      " [7.39070367e-06 1.98982949e-02 9.80988254e-01]\n",
      " [1.25418357e-03 9.83991596e-01 1.27997910e-02]\n",
      " [1.50035736e-03 9.86322289e-01 1.05207181e-02]\n",
      " [1.39361388e-03 9.87193568e-01 1.01676176e-02]\n",
      " [1.38261052e-03 9.87283082e-01 1.01288425e-02]\n",
      " [1.50576439e-03 9.86358723e-01 1.04832226e-02]\n",
      " [1.39098464e-03 9.87231886e-01 1.01464297e-02]\n",
      " [1.39231776e-03 9.87161310e-01 1.01933467e-02]\n",
      " [1.39986462e-03 9.87102071e-01 1.02180637e-02]\n",
      " [1.39816085e-03 9.87180585e-01 1.01665662e-02]\n",
      " [1.43586372e-03 9.86896347e-01 1.02796558e-02]\n",
      " [1.39971753e-03 9.87161720e-01 1.01763710e-02]\n",
      " [1.52239485e-03 9.86230268e-01 1.05296295e-02]\n",
      " [1.43141041e-03 9.86927548e-01 1.02681807e-02]\n",
      " [1.40630356e-03 9.87118699e-01 1.01917689e-02]\n",
      " [1.55311086e-03 9.85994977e-01 1.06119047e-02]\n",
      " [1.41431094e-03 9.87058122e-01 1.02161327e-02]\n",
      " [7.31270217e-06 1.96102960e-02 9.81269661e-01]\n",
      " [7.31399583e-06 1.96137829e-02 9.81266253e-01]\n",
      " [7.31232196e-06 1.96094358e-02 9.81270502e-01]\n",
      " [7.31430947e-06 1.96141406e-02 9.81265902e-01]\n",
      " [7.31243419e-06 1.96096841e-02 9.81270259e-01]\n",
      " [7.31193671e-06 1.96085454e-02 9.81271372e-01]\n",
      " [7.35063969e-06 1.97411760e-02 9.81141777e-01]\n",
      " [7.31223451e-06 1.96092339e-02 9.81270699e-01]\n",
      " [7.31198329e-06 1.96086689e-02 9.81271252e-01]\n",
      " [7.31375050e-06 1.96126998e-02 9.81267310e-01]\n",
      " [7.37325291e-06 1.98242391e-02 9.81060611e-01]\n",
      " [7.31345189e-06 1.96131732e-02 9.81266850e-01]\n",
      " [7.31308481e-06 1.96114039e-02 9.81268578e-01]\n",
      " [7.31242931e-06 1.96097694e-02 9.81270176e-01]\n",
      " [7.31231774e-06 1.96094165e-02 9.81270521e-01]\n",
      " [7.31462213e-06 1.96147405e-02 9.81265315e-01]\n",
      " [7.31782048e-06 1.96249507e-02 9.81255338e-01]\n",
      " [7.31643580e-06 1.96188495e-02 9.81261298e-01]\n",
      " [7.31180768e-06 1.96082497e-02 9.81271661e-01]\n",
      " [7.39979099e-06 1.99387800e-02 9.80948697e-01]\n",
      " [7.31290067e-06 1.96107583e-02 9.81269209e-01]\n",
      " [7.31589216e-06 1.96184613e-02 9.81261679e-01]\n",
      " [7.31185777e-06 1.96083645e-02 9.81271549e-01]\n",
      " [7.62238364e-06 2.07904158e-02 9.80115942e-01]\n",
      " [7.31607976e-06 1.96180724e-02 9.81262057e-01]\n",
      " [7.31558035e-06 1.96175823e-02 9.81262538e-01]\n",
      " [8.61017906e-06 2.48190920e-02 9.76165634e-01]\n",
      " [7.81066533e-06 2.15097772e-02 9.79411821e-01]\n",
      " [7.31226239e-06 1.96092933e-02 9.81270641e-01]\n",
      " [7.35068105e-06 1.97494754e-02 9.81133679e-01]\n",
      " [7.31204306e-06 1.96088090e-02 9.81271115e-01]\n",
      " [7.32458503e-06 1.96377528e-02 9.81242815e-01]\n",
      " [7.31212559e-06 1.96089782e-02 9.81270949e-01]\n",
      " [1.33583274e-05 4.81600597e-02 9.53020869e-01]\n",
      " [7.31659982e-06 1.96226451e-02 9.81257594e-01]\n",
      " [7.31193339e-06 1.96085389e-02 9.81271379e-01]\n",
      " [7.31566366e-06 1.96170645e-02 9.81263043e-01]\n",
      " [7.32100673e-06 1.96320844e-02 9.81248363e-01]\n",
      " [8.47471772e-06 2.42157738e-02 9.76758189e-01]\n",
      " [7.31540677e-06 1.96187769e-02 9.81261374e-01]\n",
      " [7.31241479e-06 1.96096411e-02 9.81270301e-01]\n",
      " [7.31750233e-06 1.96273047e-02 9.81253043e-01]\n",
      " [7.31399583e-06 1.96137829e-02 9.81266253e-01]\n",
      " [7.31267881e-06 1.96102449e-02 9.81269711e-01]\n",
      " [7.31284747e-06 1.96106308e-02 9.81269333e-01]\n",
      " [7.31296936e-06 1.96112594e-02 9.81268720e-01]\n",
      " [7.31963901e-06 1.96368580e-02 9.81243713e-01]\n",
      " [7.32018038e-06 1.96351659e-02 9.81245361e-01]\n",
      " [7.31981133e-06 1.96265382e-02 9.81253780e-01]\n",
      " [7.33520821e-06 1.96784830e-02 9.81203024e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Import Liabraries\n",
    "import numpy as np # Linear Algebra Matrices\n",
    "import pandas as pd # Dataframe Creation Manipulation\n",
    "import matplotlib.pyplot as plt # Plotting\n",
    "# from NNDeep import DeepNN\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_excel (r'fisheriris.xlsx')\n",
    "names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "idx = [50,100,150]\n",
    "\n",
    "# {50/100, 75/75, 100/50, 125/25}\n",
    "# first four columns are \n",
    "# X = np.zeros((150,4))\n",
    "X = np.array(df.values[:,0:4], dtype=np.float32)\n",
    "T = np.zeros((150,3))\n",
    "T[0:50,0] = 1 # setosa\n",
    "T[50:100,1] = 1 #versicolor\n",
    "T[100:150,2] = 1 #virginica\n",
    "\n",
    "\n",
    "NN = DeepNN([4,2,4,2,3])\n",
    "# NN.learn(2000,1e-5,X[:,1:],y,['tanh','identity','identity'], 'least_squares')\n",
    "NN.learn(1_000,1e-1,X,T,['sigmoid','sigmoid','sigmoid','sigmoid'], 'least_squares', 100, 'sgd', 100)\n",
    "\n",
    "print(NN.think(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
