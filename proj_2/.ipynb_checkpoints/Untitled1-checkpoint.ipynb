{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# A class that models the Neural Net with L-layers and\n",
    "# N neurons in each layer. It also contains the functions\n",
    "# for training, testing, and optimizing the Neural Network\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "class DeepNN:\n",
    "\n",
    "    # Constructor to build the structure of the Neural Network\n",
    "    # It accepts the layers in the format of [2,3,1] -> 2 Neuron Input Layer,\n",
    "    # 3 Neuron Hidden Layer and 1 Neuron output layer\n",
    "    def __init__(self, layers, activations, epsilon = 0.55):\n",
    "        ############################### Initialize the number of layers and neurons\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.hidden_layers = len(layers) - 2\n",
    "        self.input_neurons = layers[0]\n",
    "        self.output_neurons = layers[-1]\n",
    "\n",
    "        ########## Intialize parameters for Forward Propogation\n",
    "        # Initialize Weights\n",
    "        self.epsilon = epsilon  # Random Weight Initialization Factor\n",
    "        self.weights = []\n",
    "        for i in range(self.num_layers-2):\n",
    "            self.weights.append(np.random.randn(layers[i]+1, layers[i+1]+1)*2*self.epsilon - self.epsilon)\n",
    "                        # We add a +1 to incorporate for weights from the +1 neuron for the bias\n",
    "        self.weights.append(np.random.randn(layers[-2]+1, layers[-1])*2*self.epsilon - self.epsilon)\n",
    "\n",
    "        self.a = [] # To keep track of activations\n",
    "        self.z = [] # To keep track of layer values\n",
    "        self.activations = activations # Activations for each layer\n",
    "\n",
    "        ######### Intialize parameters for Backward Propogation\n",
    "        self.delta = []\n",
    "        self.gradient = []\n",
    "\n",
    "        # Initialize Scaling\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    ################################### Define Some Activation Functions and their derivatives ##################\n",
    "    def sigmoid(self,x):\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "    def sigmoidPrime(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "\n",
    "    def reLU(self,x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def reLUPrime(self,x):\n",
    "        return np.where(x > 0, 1.0, 0.0)\n",
    "\n",
    "    def softmax(self,x):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "    def tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_prime(self,x):\n",
    "        return 1 - np.power(np.tanh(x),2)\n",
    "\n",
    "    def identity(self,x):\n",
    "        return x\n",
    "\n",
    "    def identity_prime(self,x):\n",
    "        return 1\n",
    "\n",
    "    ######################################### Cost Functions #############################################\n",
    "    # Least Squares\n",
    "    def least_squares_cost(self,t):\n",
    "        return 0.5*np.mean( (t-self.a[-1])**2 )\n",
    "\n",
    "    # Cross Entropy Log Loss Function\n",
    "    def log_loss(self,t):\n",
    "        return np.mean( np.nan_to_num( -1*t*np.log(self.a[-1]) - (1-t)*np.log(1-self.a[-1]) ) )\n",
    "\n",
    "    ######################################### Forward Feed ##############################################\n",
    "    def forwardFeed(self, X):\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "\n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "#         print(a)\n",
    "        for i in range(self.num_layers-1):\n",
    "#             print(a[i])\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(self.activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(self.activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(self.activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(self.activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(self.activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "        self.a = a\n",
    "        self.z = z\n",
    "\n",
    "    def backPropogate(self,y):\n",
    "\n",
    "        delta = []\n",
    "        gradient = []\n",
    "        weights_flipped = self.weights[::-1]\n",
    "        z_flipped = self.z[::-1]\n",
    "        activations_flipped = self.a[::-1]\n",
    "        activation_func_flipped = self.activations[::-1]\n",
    "        delta.append(activations_flipped[0] - y)\n",
    "        for i in range(0,self.num_layers-2):\n",
    "                if(activation_func_flipped[i] == 'sigmoid'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.sigmoidPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'reLU'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.reLUPrime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'tanh'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.tanh_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'identity'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T ) * self.identity_prime(z_flipped[i+1]) )\n",
    "                elif(activation_func_flipped[i] == 'softmax'):\n",
    "                    delta.append( np.dot(delta[i], weights_flipped[i].T))\n",
    "\n",
    "        delta = delta[::-1]\n",
    "\n",
    "        for i in range(len(delta)):\n",
    "            gradient.append( np.dot(self.a[i].T, delta[i]) )\n",
    "            \n",
    "        self.delta = delta\n",
    "        self.gradient = gradient\n",
    "\n",
    "    def learn(self, epochs, learning_rate, X, y, cost_func, metrics_at=10, optimizer = '', batch_size=10, scaler_type='standard_scaler', split=False, test_size=0.25, beta = 0.9, beta_2 = 0.99, epsilon=10e-8):\n",
    "        start = time.time()\n",
    "        \n",
    "        cost=[]\n",
    "        \n",
    "        v_n = []\n",
    "        s_n = []\n",
    "        for j in range(len(self.weights)):\n",
    "            v_n.append(np.zeros(self.weights[j].shape))\n",
    "            s_n.append(np.zeros(self.weights[j].shape))\n",
    "            \n",
    "        if scaler_type == 'min_max_scaler':\n",
    "            self.scaler = preprocessing.MinMaxScaler()\n",
    "        X = self.scaler.fit_transform(X)\n",
    "\n",
    "        if split:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)\n",
    "            X = X_train\n",
    "            y = y_train\n",
    "\n",
    "        \n",
    "        for i in range(epochs):\n",
    "                if optimizer == 'mini_batch':\n",
    "                    # Batch size of 1 makes it SGD\n",
    "                    no_of_batches = int(len(X)/batch_size)\n",
    "                    for k in range(no_of_batches):\n",
    "                        if k==no_of_batches-1:\n",
    "                            X_sgd = X[k*batch_size:]\n",
    "                            y_sgd = y[k*batch_size:]\n",
    "                        else:\n",
    "                            X_sgd = X[k*batch_size:(k+1)*batch_size]\n",
    "                            y_sgd = y[k*batch_size:(k+1)*batch_size]\n",
    "                        self.forwardFeed(X_sgd)\n",
    "                        self.backPropogate(y_sgd)\n",
    "                        for j in range(len(self.gradient)):\n",
    "                            v_n[j] = beta * v_n[j] + (1-beta) * self.gradient[j]\n",
    "                            self.weights[j] = self.weights[j] - learning_rate * v_n[j]\n",
    "                elif optimizer == 'rmsprop':\n",
    "                    # Batch size of 1 makes it SGD\n",
    "                    no_of_batches = int(len(X)/batch_size)\n",
    "                    for k in range(no_of_batches):\n",
    "                        if k==no_of_batches-1:\n",
    "                            X_sgd = X[k*batch_size:]\n",
    "                            y_sgd = y[k*batch_size:]\n",
    "                        else:\n",
    "                            X_sgd = X[k*batch_size:(k+1)*batch_size]\n",
    "                            y_sgd = y[k*batch_size:(k+1)*batch_size]\n",
    "                            \n",
    "                        self.forwardFeed(X_sgd)\n",
    "                        self.backPropogate(y_sgd)\n",
    "                        for j in range(len(self.gradient)):\n",
    "                            s_n[j] = beta_2 * s_n[j] + (1-beta_2) * np.power(self.gradient[j],2)\n",
    "                            self.weights[j] = self.weights[j] - learning_rate * self.gradient[j]/np.sqrt(s_n[j]+epsilon)\n",
    "                elif optimizer == 'adam':\n",
    "                    # Batch size of 1 makes it SGD\n",
    "                    no_of_batches = int(len(X)/batch_size)\n",
    "                    for k in range(no_of_batches):\n",
    "                        if k==no_of_batches-1:\n",
    "                            X_sgd = X[k*batch_size:]\n",
    "                            y_sgd = y[k*batch_size:]\n",
    "                        else:\n",
    "                            X_sgd = X[k*batch_size:(k+1)*batch_size]\n",
    "                            y_sgd = y[k*batch_size:(k+1)*batch_size]\n",
    "                        self.forwardFeed(X_sgd)\n",
    "                        self.backPropogate(y_sgd)\n",
    "                        for j in range(len(self.gradient)):\n",
    "                            v_n[j] = beta * v_n[j] + (1-beta) * self.gradient[j]\n",
    "                            s_n[j] = beta_2 * s_n[j] + (1-beta_2) * np.power(self.gradient[j],2)\n",
    "                            \n",
    "                            # Beta Correction\n",
    "                            v_n[j] = v_n[j] / (1-np.power(beta,k)+epsilon)\n",
    "                            s_n[j] = s_n[j] / (1-np.power(beta_2,k)+epsilon)\n",
    "                            \n",
    "                            self.weights[j] = self.weights[j] - learning_rate * v_n[j]/np.sqrt(s_n[j]+epsilon)\n",
    "\n",
    "                else:\n",
    "                    self.forwardFeed(X)\n",
    "                    self.backPropogate(y)\n",
    "                    for j in range(len(self.gradient)):\n",
    "                        self.weights[j] = self.weights[j] - learning_rate * self.gradient[j]\n",
    "\n",
    "                if(i%metrics_at == 0):\n",
    "                    self.forwardFeed(X)\n",
    "                    print('Effective epoch: ', i/metrics_at + 1)\n",
    "                    if(cost_func == 'log_loss'):\n",
    "                        cost.append(self.log_loss(y))\n",
    "                        print('Accuracy: ', np.mean(np.round(self.think(X))==y) * 100, '%')\n",
    "                    elif(cost_func == 'least_squares'):\n",
    "                        cost.append(self.least_squares_cost(y))\n",
    "                    print('Cost: ', cost[-1], '\\n')\n",
    "\n",
    "        \n",
    "\n",
    "        end = time.time()\n",
    "        \n",
    "        \n",
    "        if(cost_func == 'log_loss' and split):\n",
    "            print('Testing Accuracy: ', np.mean(np.round(self.think(X_test))==y_test) * 100, '%')\n",
    "        plt.plot(cost)\n",
    "        print('Time Taken: ', end-start, ' seconds')\n",
    "        return self.weights\n",
    "\n",
    "    def think(self,X, scale=True):\n",
    "        if scale:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        a = [X] # Keep Track of activations\n",
    "        z = []\n",
    "\n",
    "        # Add Bias\n",
    "        c = np.ones([1,a[0].shape[0]]).reshape(a[0].shape[0],1)\n",
    "        a[0] = np.concatenate((c,a[0]), axis=1)\n",
    "#         print(a)\n",
    "        for i in range(self.num_layers-1):\n",
    "#             print(a[i])\n",
    "            z.append(np.dot(a[i],self.weights[i]))\n",
    "            if(self.activations[i] == 'sigmoid'):\n",
    "                a.append(self.sigmoid(z[i]))\n",
    "            elif(self.activations[i] == 'reLU'):\n",
    "                a.append(self.reLU(z[i]))\n",
    "            elif(self.activations[i] == 'tanh'):\n",
    "                a.append(self.tanh(z[i]))\n",
    "            elif(self.activations[i] == 'softmax'):\n",
    "                a.append(self.softmax(z[i]))\n",
    "            elif(self.activations[i] == 'identity'):\n",
    "                a.append(self.identity(z[i]))\n",
    "                \n",
    "        return a[-1]\n",
    "\n",
    "\n",
    "# In[83]:\n",
    "\n",
    "\n",
    "########################## Part a) - Generate Dataset\n",
    "X = np.array([[-1, -1],[1, 1],[-1, 1],[1, -1]],dtype='float')\n",
    "t = np.array([1, 1, 0, 0],dtype='float').reshape(4,1)\n",
    "\n",
    "\n",
    "NN = DeepNN([2,2,1], activations=['tanh','sigmoid'], epsilon = 0.12)\n",
    "\n",
    "w = NN.learn(epochs=1500, learning_rate=0.1, X=X, y=t, cost_func='log_loss', metrics_at=1, split=False)\n",
    "plt.title('XOR Cost Function')\n",
    "print('Accuracy: ',np.mean(np.round(NN.think(X))==t) * 100)\n",
    "\n",
    "\n",
    "x_mesh = np.linspace(-1.2,1.2,30).reshape(30,1)\n",
    "y_mesh = np.linspace(-1.2,1.2,30).reshape(30,1)\n",
    "\n",
    "X_mesh, Y_mesh = np.meshgrid(x_mesh,x_mesh)\n",
    "X_test = np.hstack((x_mesh,y_mesh))\n",
    "\n",
    "\n",
    "\n",
    "# Classify Points for Decision Boundary\n",
    "predicitions = np.zeros([len(x_mesh),len(y_mesh)])\n",
    "for i in range(len(X_mesh)):\n",
    "    for j in range(len(Y_mesh)):\n",
    "        pnt = np.array([X_mesh[i,j],Y_mesh[i,j]]).reshape(1,2)\n",
    "        predicitions[i,j] = NN.think(pnt, scale=False)[0,0]\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.contour3D(X_mesh, Y_mesh, predicitions-0.5,1000)\n",
    "\n",
    "x_xor = [-1, 1, -1, 1]\n",
    "y_xor = [-1, 1, 1, -1]\n",
    "t_xor = [0.5, 0.5, -0.5, -0.5]\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.scatter(x_xor, y_xor, t_xor, c='r')\n",
    "plt.title('XOR Decision Surface using Logistic Sigmoid')\n",
    "\n",
    "\n",
    "# Part - b Generate Dataset\n",
    "plt.figure()\n",
    "np.random.seed(100)\n",
    "X = np.sort(2*np.random.rand(50,1)-1)\n",
    "t = np.sin(2*(np.pi)*X) + 0.3*np.random.randn(50,1)\n",
    "\n",
    "# Part - b Regression\n",
    "np.random.seed(100)\n",
    "X = np.sort(2*np.random.rand(50,1)-1)\n",
    "t = np.sin(2*(np.pi)*X) + 0.3*np.random.randn(50,1)\n",
    "\n",
    "\n",
    "# Build NN with 20 hidden units\n",
    "NN2 = DeepNN([1,2,1], activations=['tanh','identity'],epsilon=0.9)\n",
    "w = NN2.learn(epochs=50000, learning_rate=0.001, X=X, y=t, cost_func='least_squares', metrics_at=10000, optimizer='rmsprop',batch_size=50)\n",
    "plt.title('Regression with 20 Hidden Units Cost Function')\n",
    "\n",
    "# plot the model\n",
    "x_mesh = np.linspace(np.min(X),np.max(X),500).reshape(500,1)\n",
    "y = NN2.think(x_mesh)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_mesh,y,'r')\n",
    "plt.scatter(X,t)\n",
    "plt.title('Regression with 20 hidden Units')\n",
    "\n",
    "# Build NN with 20 hidden units\n",
    "plt.figure()\n",
    "NN2 = DeepNN([1,20,1], activations=['tanh','identity'],epsilon=2.1)\n",
    "w = NN2.learn(epochs=10000, learning_rate=0.001, X=X, y=t, cost_func='least_squares', metrics_at=1000, optimizer='rmsprop',batch_size=5)\n",
    "plt.title('Regression with 20 Hidden Units Cost Function')\n",
    "\n",
    "# plot the model\n",
    "x_mesh = np.linspace(np.min(X)-0.25,np.max(X)+0.25,500).reshape(500,1)\n",
    "y = NN2.think(x_mesh)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_mesh, y, 'r')\n",
    "plt.scatter(X,t)\n",
    "plt.title('Regression with 20 hidden Units')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
