{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=20):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        Z.insert(i, W[i] @ A[i] + B[i])\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #comuns are the samples now\n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1)\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "#         dJ_dW.insert(i+1, error[i+1] @ A[i+1].T)\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "#         error.insert(i, error_dummy)\n",
    "        error[i] = error_dummy\n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    \n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "def train(X, T, hidden_layers=[2], epochs=500,\\\n",
    "          rho=.1, normalize_data=False, show_cost=0, seed=1):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features,\\\n",
    "                                      no_outputs, hidden_layers, seed=1)    \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))    \n",
    "\n",
    "    N = int(np.round(epochs/20))\n",
    "    i_for_show_cost = np.round(np.linspace(0,epochs,N))\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "        \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    #loop variables, along with other variables and lists (cost, accuracy)\n",
    "    # that are returned by the function\n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    iter_passed = []\n",
    "    match = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    k_cost = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW = backprop(W, Z, A, Y_hat, T.T)\n",
    "        #grad descent\n",
    "        for a in range(len(W)):\n",
    "            W[a] = W[a] - rho*dJ_dW[a]\n",
    "        \n",
    "        if is_equal_in_boolean_terms(Y_hat.T, T):\n",
    "            match += 1            \n",
    "            if match > 5 and match < 7:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "        if match > 100: #seeing the match 100 times \n",
    "            converged = True #makes the predictions more confident\n",
    "            break\n",
    "            \n",
    "        cost_final.insert(i,cost_MSE(T, Y_hat.T))\n",
    "        accuracy.insert(i,calc_accuracy(T, Y_hat.T))\n",
    "        iter_passed.insert(i,i)\n",
    "        if show_cost and i_for_show_cost[k_cost] == i:\n",
    "            print(\"   \" + str(i) + \" iteration, accuracy: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\"   cost: \" + str(cost_final[i]))\n",
    "            k_cost += 1        \n",
    "        \n",
    "    if converged:\n",
    "        print(\"   Converged in \" + str(idx_done) + \" iterations\")\n",
    "    else:\n",
    "        print(\"   Did not converge.\")\n",
    "    print()\n",
    "    if show_cost:\n",
    "        plt.scatter(iter_passed, cost_final, s=1, color=\"red\")\n",
    "        plt.title(\"iterations X cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        \n",
    "    return [W, B, Y_hat.T, X, cost_final, epochs,\\\n",
    "            idx_done, converged, rho, normalize_data, accuracy]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy]\n",
    "def predict2(W, B, X, T, normalized=0):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy]\n",
    "def predict(X, T, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "def is_equal_in_boolean_terms(Y_hat, T):\n",
    "    return np.array_equal(np.round(Y_hat), T)\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN functions 2 (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, hidden_layers=[3], iterations=500, rho=.01, normalize_data=False, print_cost=0):\n",
    "    train_pkg_all_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # hidden_layers = list of number of units for each layer. Minimum 1 [1]\n",
    "                #train_gates[i] : [W, B, Y, X, cost_final, epochs, idx_done, \\\n",
    "                            #  converged, rho, normalize_data, accuracy]\n",
    "        train_pkg_all_gates[i] = train(X, t[i],\\\n",
    "                                       hidden_layers = hidden_layers, \\\n",
    "                               epochs=iterations, rho=rho, \\\n",
    "                                       normalize_data=normalize_data, \\\n",
    "                                       show_cost=print_cost, seed=19)\n",
    "\n",
    "    return train_pkg_all_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_logic_gate >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_logic_gate(X, T, train_pkg):\n",
    "    #y_hat is returned transposed already from predict\n",
    "    Y_hat, accuracy = predict(X, T, train_pkg)\n",
    "    \n",
    "#   train_pkg : [W, B, Y, X, cost_final, epochs, idx_done,\\\n",
    "#                converged, rho, normalize_data, accuracy]\n",
    "    # indeces used, especially:\n",
    "    #                                 7. converged\n",
    "    #                                 6. idx_done, \n",
    "    #                                 5. epochs\n",
    "    #                                 8. rho\n",
    "    match_pkg = [train_pkg[7], train_pkg[6],\\\n",
    "                  train_pkg[5], train_pkg[8],\\\n",
    "                  is_equal_in_boolean_terms(Y_hat, T), Y_hat]\n",
    "    return match_pkg\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_all_gate_outputs(X, T, train_pkg_all_gates):\n",
    "    matches = {}\n",
    "\n",
    "    for i in t:\n",
    "        matches[i] = match_logic_gate(X, T[i], train_pkg_all_gates[i])\n",
    "\n",
    "    return matches\n",
    "\n",
    "def print_match(match):\n",
    "    print(i + \" converged: \" + str(match[0]))\n",
    "    print(\"===========================================\")\n",
    "    print(\"  iter. to converge: \" + str(match[1]))\n",
    "    print(\"  iter. max: \" + str(match[2]))\n",
    "    if match[4]:\n",
    "        print(\"  ==== CORRECT prediction ==== \")\n",
    "    else:\n",
    "        print(\"  ==== INCORRECT prediction ==== \")\n",
    "    print(\"  predicted y (y_hat): \")\n",
    "    print(np.round(match[5]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset / targets\n",
    "X: possible inputs of a logic function.\n",
    "t: dictionary with possible outputs for each logic gates. \n",
    "    4 binary ouputs to match NN's output probabilities of 0 or 1. \n",
    "    - if [p(0) p(1)] == [1 0] then probability of 0 == 1 && probability of 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NAND\": np.array([[0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"OR\": np.array([[1, 0],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NOR\": np.array([[0, 1],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"XOR\": np.array([[1, 0],\\\n",
    "                     [0, 1],\\\n",
    "                     [0, 1],\\\n",
    "                     [1, 0]], dtype=np.float32) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN run\n",
    "\n",
    "The Neural Network can be run with train() and then feeding the train_package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 4 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 2\n",
      "   learnable weights: 32\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 0.0%\n",
      "   cost: 0.5317253714411352\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.2522184507254147\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.15257249312022964\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.06953169354353894\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.03618848794526732\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.02234095852275065\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.015409225392533796\n",
      "   Converged in 44 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 4 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 2\n",
      "   learnable weights: 32\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 100.0%\n",
      "   cost: 0.4696811591358406\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.20269777730160388\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.08382588860275872\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.03820715760142522\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.021803245040314355\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.014413125831230868\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.010456687484692818\n",
      "   Converged in 29 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 4 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 2\n",
      "   learnable weights: 32\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 50.0%\n",
      "   cost: 0.498662367453295\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.308738924788579\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.17072570676538884\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.07306204634502551\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.03379859793204053\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.019532514579866225\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.013059722721113694\n",
      "   Converged in 38 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 4 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 2\n",
      "   learnable weights: 32\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 50.0%\n",
      "   cost: 0.5027441631236806\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.2806284212831905\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.1308182448256531\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.05367202457576324\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.026031603552099037\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.015811204019477602\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.010963107919081824\n",
      "   Converged in 33 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 4 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 2\n",
      "   learnable weights: 32\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 75.0%\n",
      "   cost: 0.49851964402261284\n",
      "   20 iteration, accuracy: 50.0%\n",
      "   cost: 0.4960234950117989\n",
      "   40 iteration, accuracy: 50.0%\n",
      "   cost: 0.4925412504928831\n",
      "   60 iteration, accuracy: 75.0%\n",
      "   cost: 0.4854125957803643\n",
      "   80 iteration, accuracy: 75.0%\n",
      "   cost: 0.4680349643439137\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.42145253697113816\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.31495804162034224\n",
      "   140 iteration, accuracy: 100.0%\n",
      "   cost: 0.19101207435229606\n",
      "   160 iteration, accuracy: 100.0%\n",
      "   cost: 0.11222906255465148\n",
      "   180 iteration, accuracy: 100.0%\n",
      "   cost: 0.07030062483328524\n",
      "   Converged in 98 iterations\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pkg_all_gates = train_all_gates(X, t, hidden_layers=[4,4],\\\n",
    "                    iterations=10_000, normalize_data=True, rho=1, print_cost=1)\n",
    "\n",
    "# (X, t, hidden_layers=[3], iterations=500, rho=.01, print_cost=0)\n",
    "matches = match_all_gate_outputs(X, t, train_pkg_all_gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches\n",
    "matches is a Python dictionary. For ex: \n",
    "\n",
    "    matches[\"OR\"]\n",
    "    returns a list:\n",
    "            matches[\"OR\"][0] == boolean (converged or not, True or False)\n",
    "            matches[\"OR\"][1] == idx_done, no. of iterations to converge\n",
    "            matches[\"OR\"][2] == total iterations (epochs)\n",
    "            matches[\"OR\"][3] == learning rate used(rho)\n",
    "            matches[\"OR\"][4] == matches with target (comparing both p(0) and p(1))\n",
    "            matches[\"OR\"][5] == predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 139\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "NAND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 124\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "OR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 133\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "\n",
      "NOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 128\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "XOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 193\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in matches:\n",
    "    print_match(matches[i])\n",
    "# predict(train_pkg_all_gates[\"XOR\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
