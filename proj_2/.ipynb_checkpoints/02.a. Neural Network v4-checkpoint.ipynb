{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "# from numba import vectorize\n",
    "# from numba import cuda\n",
    "from numba import autojit, jit\n",
    "# import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cuda.jit\n",
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "# @cuda.jit(device=True)\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "# @cuda.jit(device=True)\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "# @autojit\n",
    "# @cuda.jit\n",
    "# @jit(nopython=True)\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        bias = np.mean(B[i],axis=1)\n",
    "        bias = bias.reshape(bias.shape[0],1)\n",
    "        Z.insert(i, W[i] @ A[i] + bias)\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #comuns are the samples now\n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "# @cuda.jit(device=True)\n",
    "\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "# @jit(nopython=True)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1)\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    dJ_dB = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        deriv_B = np.mean(logistic_sigmoid(Z[i], derivative=1),axis=1)\n",
    "        deriv_B = deriv_B.reshape(deriv_B.shape[0],1)\n",
    "        \n",
    "        dJ_dB[i+1] = dJ_dW[i+1] @ deriv_B #not sure if this is right\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "        error[i] = error_dummy\n",
    "        \n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    dJ_dB[0] = dJ_dW[0]\n",
    "    \n",
    "    return dJ_dW, dJ_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "# @cuda.jit(device=True)\n",
    "# @autojit\n",
    "def train(X, T, hidden_layers=[2], epochs=500,\\\n",
    "          rho=.1, normalize_data=False, show_cost=0, seed=1, w_transfer=[]):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features,\\\n",
    "                                      no_outputs, hidden_layers, seed=1)\n",
    "    \n",
    "    if w_transfer:\n",
    "        W=w_transfer\n",
    "        \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))    \n",
    "\n",
    "    N = int(np.round(epochs/20))\n",
    "    i_for_show_cost = np.round(np.linspace(0,epochs,N))\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "        \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    #loop variables, along with other variables and lists (cost, accuracy)\n",
    "    # that are returned by the function\n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    iter_passed = []\n",
    "    match = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    k_cost = 0\n",
    "    acc_count = 0\n",
    "    for i in range(epochs):\n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW, dJ_dB = backprop(W, Z, A, Y_hat, T.T)\n",
    "    \n",
    "        #grad descent\n",
    "        for a in range(len(W)):\n",
    "            W[a] = W[a] - rho*dJ_dW[a]\n",
    "            B[a] = B[a] - rho*dJ_dB[a]\n",
    "        \n",
    "        if match <=5:\n",
    "            idx_done = i + 1\n",
    "        \n",
    "        if is_equal_in_boolean_terms(Y_hat.T, T):\n",
    "            match += 1            \n",
    "                \n",
    "            if match > 6 and match < 7:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            \n",
    "        if match > 20: #seeing the match 20 times \n",
    "            converged = True #makes the predictions more confident\n",
    "            break        \n",
    "        \n",
    "        cost_final.insert(i,cost_MSE(T, Y_hat.T))\n",
    "        accuracy.insert(i,calc_accuracy(T, Y_hat.T))\n",
    "        iter_passed.insert(i,i)\n",
    "            \n",
    "        if show_cost and i_for_show_cost[k_cost] == i:\n",
    "            print(\"   \" + str(i) + \" iteration, accuracy: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\"   cost: \" + str(cost_final[i]))\n",
    "            k_cost += 1        \n",
    "        \n",
    "    if converged:\n",
    "        print(\"   Converged in \" + str(idx_done) + \" iterations\")\n",
    "    else:\n",
    "        print(\"   Did not converge.\")\n",
    "    print()\n",
    "    if show_cost:\n",
    "        plt.scatter(iter_passed, cost_final, s=1, color=\"red\")\n",
    "        plt.title(\"iterations X cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        \n",
    "    return [W, B, Y_hat.T, X, cost_final, epochs,\\\n",
    "            idx_done, converged, rho, normalize_data, accuracy]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy]\n",
    "def predict(X, T, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "def is_equal_in_boolean_terms(Y_hat, T):\n",
    "    return np.array_equal(np.round(Y_hat), T)\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN functions 2 (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, hidden_layers=[3], iterations=500, rho=.01, normalize_data=False, print_cost=0):\n",
    "    train_pkg_all_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # hidden_layers = list of number of units for each layer. Minimum 1 [1]\n",
    "                #train_gates[i] : [W, B, Y, X, cost_final, epochs, idx_done, \\\n",
    "                            #  converged, rho, normalize_data, accuracy]\n",
    "        train_pkg_all_gates[i] = train(X, t[i],\\\n",
    "                                       hidden_layers = hidden_layers, \\\n",
    "                               epochs=iterations, rho=rho, \\\n",
    "                                       normalize_data=normalize_data, \\\n",
    "                                       show_cost=print_cost, seed=1)\n",
    "\n",
    "    return train_pkg_all_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_logic_gate >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_logic_gate(X, T, train_pkg):\n",
    "    #y_hat is returned transposed already from predict\n",
    "    Y_hat, accuracy = predict(X, T, train_pkg)\n",
    "    \n",
    "#   train_pkg : [W, B, Y, X, cost_final, epochs, idx_done,\\\n",
    "#                converged, rho, normalize_data, accuracy]\n",
    "    # indeces used, especially:\n",
    "    #                                 7. converged\n",
    "    #                                 6. idx_done, \n",
    "    #                                 5. epochs\n",
    "    #                                 8. rho\n",
    "    match_pkg = [train_pkg[7], train_pkg[6],\\\n",
    "                  train_pkg[5], train_pkg[8],\\\n",
    "                  is_equal_in_boolean_terms(Y_hat, T), Y_hat]\n",
    "    return match_pkg\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_all_gate_outputs(X, T, train_pkg_all_gates):\n",
    "    matches = {}\n",
    "\n",
    "    for i in t:\n",
    "        matches[i] = match_logic_gate(X, T[i], train_pkg_all_gates[i])\n",
    "\n",
    "    return matches\n",
    "\n",
    "def print_match(match):\n",
    "    print(i + \" converged: \" + str(match[0]))\n",
    "    print(\"===========================================\")\n",
    "    if match[1]:\n",
    "        print(\"  iter. to converge: \" + str(match[1]))\n",
    "    else:\n",
    "        print(\"  reached max. iter\")\n",
    "    print(\"  iter. max: \" + str(match[2]))\n",
    "    if match[4]:\n",
    "        print(\"  ==== CORRECT prediction ==== \")\n",
    "    else:\n",
    "        print(\"  ==== INCORRECT prediction ==== \")\n",
    "    print(\"  predicted y (y_hat): \")\n",
    "    print(np.round(match[5]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset / targets\n",
    "\n",
    "X: possible inputs of a binary logic function.\n",
    "\n",
    "t: dictionary with possible outputs for each logic gates. \n",
    "    4 binary ouputs to match NN's output probabilities of 0 or 1. \n",
    "    - if [p(0) p(1)] == [1 0] then probability of 0 == 1 && probability of 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0,1],\\\n",
    "              [0,1,1],\\\n",
    "              [1,0,1],\\\n",
    "              [1,1,1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NAND\": np.array([[0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"OR\": np.array([[1, 0],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NOR\": np.array([[0, 1],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0]], dtype=np.float32),\n",
    "    \n",
    "#     \"XOR\": np.array([[1, 0],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [1, 0]], dtype=np.float32) }\n",
    "\n",
    "    \"XOR\": np.array([[1, 0],\\\n",
    "                     [0, 1],\\\n",
    "                     [0, 1],\\\n",
    "                     [1, 0]], dtype=np.float32) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN run\n",
    "\n",
    "The Neural Network can be run with train() and then feeding the train_package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 6 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 24\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 25.0%\n",
      "   cost: 0.7277354812588175\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.11316975292382811\n",
      "   Converged in 8 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 6 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 24\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 75.0%\n",
      "   cost: 0.47631670105488644\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.09307815902009124\n",
      "   Converged in 14 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 6 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 24\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 75.0%\n",
      "   cost: 0.3177401166323044\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.08020065012094538\n",
      "   Converged in 10 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 6 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 24\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 25.0%\n",
      "   cost: 0.8863120656813994\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.13063500932206834\n",
      "   Converged in 17 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 6 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 24\n",
      "   max epochs: 10,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 50.0%\n",
      "   cost: 0.6048067400879249\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.46717823597406616\n",
      "   Converged in 25 iterations\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pkg_all_gates = train_all_gates(X, t, hidden_layers=[6],\\\n",
    "                    iterations=10_000, normalize_data=True, rho=1, print_cost=1)\n",
    "\n",
    "# (X, t, hidden_layers=[3], iterations=500, rho=.01, print_cost=0)\n",
    "matches = match_all_gate_outputs(X, t, train_pkg_all_gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches\n",
    "matches is a Python dictionary. For ex: \n",
    "\n",
    "    matches[\"OR\"]\n",
    "    returns a list:\n",
    "            matches[\"OR\"][0] == boolean (converged or not, True or False)\n",
    "            matches[\"OR\"][1] == idx_done, no. of iterations to converge\n",
    "            matches[\"OR\"][2] == total iterations (epochs)\n",
    "            matches[\"OR\"][3] == learning rate used(rho)\n",
    "            matches[\"OR\"][4] == matches with target (comparing both p(0) and p(1))\n",
    "            matches[\"OR\"][5] == predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 8\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "NAND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 14\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "OR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 10\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "\n",
      "NOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 17\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "XOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 25\n",
      "  iter. max: 10000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in matches:\n",
    "    print_match(matches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
