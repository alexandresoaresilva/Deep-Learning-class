{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        Z.insert(i, W[i] @ A[i] + B[i])\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #comuns are the samples now\n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = ( cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1))\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "#         dJ_dW.insert(i+1, error[i+1] @ A[i+1].T)\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "#         error.insert(i, error_dummy)\n",
    "        error[i] = error_dummy\n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    \n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "def train(X, T, hidden_layers=[2], epochs=500, rho=.1, normalize_data=True, show_cost=0):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features, no_outputs, hidden_layers)\n",
    "    \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))\n",
    "    \n",
    "    j = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "    \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW = backprop(W, Z, A, Y_hat, T.T)\n",
    "\n",
    "        #grad descent\n",
    "        for j in range(len(W)):\n",
    "            W[j] = W[j] - rho*dJ_dW[j]\n",
    "\n",
    "        Y = Y_hat.T\n",
    "\n",
    "        y_and_T_match = np.allclose(Y, T, rtol=1e-03)        \n",
    "\n",
    "        if y_and_T_match: #converged\n",
    "            j += 1 \n",
    "            if j == 3:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            if j > 100: #makes the prediction more robust \n",
    "                # ( probability considered 1 == .60 or greater )\n",
    "                converged = True\n",
    "                break\n",
    "    \n",
    "        cost_final.append(cost_MSE(T, Y_hat.T))\n",
    "        accuracy.append(calc_accuracy(T, Y_hat.T))\n",
    "        \n",
    "        \n",
    "        if show_cost:\n",
    "            print(str(i) + \": accur: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\" cost: \" + str(cost_final[i]))\n",
    "    \n",
    "    if show_cost:\n",
    "        display_NN_info()\n",
    "    \n",
    "    plt.scatter(range(epochs), cost_final, s=1, color=\"red\")\n",
    "    plt.title(\"iterations X cost\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"cost\")\n",
    "    \n",
    "    print(\"final train accuracy: \" + str(accuracy[i]))\n",
    "    print()\n",
    "    \n",
    "    return W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho]\n",
    "def predict(X, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "#     accuracy = calc_accuracy(X, Y_hat)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, accuracy\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_classes(X, t, no_hidden_units=2,\\\n",
    "                    iterations=500, rho=.01, print_cost=0):\n",
    "    train_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # NO_UNITS_L1 = 6  yields max matches with rho = 1 and epochs = 500\n",
    "#         train_gates[i] : [W1, b1, W2, b2, X, Y, idx_done, epochs, converged, rho]\n",
    "        train_gates[i] = train(X, t[i], NO_UNITS_L1=no_hidden_units,\\\n",
    "                               epochs=iterations, learning_rate=rho,\\\n",
    "                               show_cost=print_cost)\n",
    "    return train_gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset (Iris) - includes function to select test and train sets\n",
    "\n",
    "    T[0:50,0]: setosa\n",
    "    T[50:100,1]: versicolor\n",
    "    T[100:150,2]:virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'fisheriris.xlsx')\n",
    "names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "idx = [50,100,150]\n",
    "\n",
    "# first four columns are \n",
    "X = np.array(df.values[:,0:4], dtype=np.float32)\n",
    "T = np.zeros((150,3))\n",
    "T[0:50,0] = 1 # setosa\n",
    "T[50:100,1] = 1 #versicolor\n",
    "T[100:150,2] = 1 #virginica\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> select_train_test_samples >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def select_train_test_samples(X, T, no_samples_train):\n",
    "    size = X.shape[0]\n",
    "\n",
    "    last_index = size-1\n",
    "    \n",
    "    all_indeces = np.linspace(0, last_index, size, dtype=np.int16)\n",
    "    \n",
    "    samples = []\n",
    "    labels = []\n",
    "    idx = []\n",
    "    \n",
    "    idx.append(random.sample(range(size), no_samples_train))\n",
    "    idx.append(np.delete(all_indeces, idx)) # delete returns a different value every time\n",
    "    \n",
    "    for i in range(2):\n",
    "        samples.insert(i, X[idx[i],:]) # samples[0] is train, [1] are test\n",
    "        labels.insert(i, T[idx[i],:]) # same for labels\n",
    "    \n",
    "    return [samples[0], samples[1], labels[0], labels[1]]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> train_random_samples >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_random_samples(X,T, no_of_train_samples, hidden_layers=[2],\\\n",
    "                         epochs=1000, rho=.1, normalize_data=True, show_cost=0):\n",
    "    [X_train, X_test, T_train,T_test] = select_train_test_samples(X, T, no_of_train_samples)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_pkg = train(X_train, T_train, hidden_layers=hidden_layers, epochs=epochs, rho=rho, show_cost=show_cost)\n",
    "    end = time.time()\n",
    "    print('Time Taken: ', end-start, ' seconds')\n",
    "    \n",
    "    Y_hat, accuracy = predict(X_test, train_pkg)\n",
    "    return accuracy, train_pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 28\n",
      "   max epochs: 1,000\n",
      "   learning rate(rho): 0.1\n",
      "\n",
      "final train accuracy: 97.33333333333334\n",
      "Time Taken:  4.236480474472046  seconds\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 28\n",
      "   max epochs: 1,000\n",
      "   learning rate(rho): 0.1\n",
      "\n",
      "final train accuracy: 98.66666666666667\n",
      "Time Taken:  4.228211164474487  seconds\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 28\n",
      "   max epochs: 1,000\n",
      "   learning rate(rho): 0.1\n",
      "\n",
      "final train accuracy: 100.0\n",
      "Time Taken:  4.222949504852295  seconds\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   layer 3: 4 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 28\n",
      "   max epochs: 1,000\n",
      "   learning rate(rho): 0.1\n",
      "\n",
      "final train accuracy: 98.66666666666667\n",
      "Time Taken:  4.224948883056641  seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_train_samples = [50, 75, 100, 125]\n",
    "accuracys = {}  #dictionary indexed with number of train samples used\n",
    "train_pkgs = {} #same as previous\n",
    "for i in range(len(no_train_samples)):\n",
    "    accuracys[no_train_samples[i]], train_pkgs[no_train_samples[i]] = \\\n",
    "                            train_random_samples(X,T, 75, hidden_layers=[2,4],\\\n",
    "                                     epochs=1000, rho=.1, normalize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
