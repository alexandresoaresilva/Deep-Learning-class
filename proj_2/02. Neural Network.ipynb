{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from scipy.special import softmax\n",
    "# from numpy import zeros, ones, eye\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_A(W1, b1, W2, b2, X):\n",
    "    A1 = (X @ W1 + b1)[0]\n",
    "    Z1 = logistic_sigmoid(A1) \n",
    "    A2 = (Z1 @ W2 + b2)[0]\n",
    "    Y = softmax_vector(A2)\n",
    "    return A1, A2, Y\n",
    "\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):\n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "\n",
    "def softmax_vector(x, derivative=0):\n",
    "    x_exp = np.exp(x)\n",
    "    \n",
    "    if derivative: #this works only for 2-element vectors\n",
    "        e = np.array([0, 0])\n",
    "        e[:] = x_exp[0]*x_exp[1]/np.sum(x_exp)\n",
    "        return e\n",
    "    \n",
    "    return np.exp(x)/np.sum(np.exp(x))\n",
    "\n",
    "def backprop_A(W2, A1, A2, X, Y, t):\n",
    "    grad_mid_layer = (t-Y) @ -softmax_vector(A2, derivative=1).T * logistic_sigmoid(A1, derivative=1) @ W2 @ X.T\n",
    "    grad_output = (t-Y) @ -softmax_vector(A2, derivative=1).T * logistic_sigmoid(A1, derivative=0) \n",
    "    \n",
    "    grad_output = grad_output @ np.eye(grad_output.shape[0],M=2)\n",
    "    return grad_mid_layer, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0], [1, 0], [1, 0], [0, 1]], dtype=np.float32),\n",
    "    \"NAND\": np.array([[0, 1], [0, 1], [0, 1], [1, 0]], dtype=np.float32),\n",
    "    \"OR\": np.array([[1, 0], [0, 1], [0, 1], [0, 1]], dtype=np.float32),\n",
    "    \"NOR\": np.array([[0, 1], [1, 0], [1, 0], [1, 0]], dtype=np.float32),\n",
    "    \"XOR\": np.array([[1, 0], [0, 1], [0, 1], [1, 0]], dtype=np.float32) \n",
    "}\n",
    "\n",
    "#learning reates\n",
    "# RHO = np.array([.0001,.001,.01,.01,.1,1], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_UNITS_L1 = 2\n",
    "W1 = np.random.randn(2,NO_UNITS_L1)\n",
    "b1 = np.zeros((1,NO_UNITS_L1))\n",
    "W2 = np.random.randn(NO_UNITS_L1,2) # 2 outputs, P(0) and P(1)\n",
    "b2 = np.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = RHO[-2]\n",
    "for i in range(10_000):\n",
    "    for j in range(len(X[:,1])):\n",
    "        A1, A2, Y = forward_prop_A(W1, b1, W2, b2, X[j,:])\n",
    "        grad_mid_layer, grad_output = backprop_A(W2, A1, A2, X[j,:], Y, t[\"AND\"][j,:])\n",
    "\n",
    "        W1 = W1 - rho*grad_mid_layer\n",
    "        W2 = W2 - rho*grad_output.T\n",
    "        b1 = b1 - rho*np.mean(grad_mid_layer)\n",
    "        b2 = b2 - rho*np.mean(grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: [1. 0.]\n",
      "Y: [0.28810017 0.71189983]\n"
     ]
    }
   ],
   "source": [
    "test = 2\n",
    "A1, A2, Y = forward_prop_A(W1, b1, W2, b2, X[test,:])\n",
    "# print(\"X: \" + str(X[test,:]))\n",
    "print(\"t: \" + str(t[\"AND\"][test,:]))\n",
    "print(\"Y: \" + str(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (t[\"AND\"][test,:]-Y) @ -softmax_vector(A2, derivative=1).T #* logistic_sigmoid(A1, derivative=1) @ W2 @ X.T\n",
    "-softmax_vector(A2, derivative=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 @ X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(A1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
