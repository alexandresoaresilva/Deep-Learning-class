{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost(y[1,:].reshape(1,2),y_hat[1,:].reshape(1,2), derivative=0)\n",
    "def cost(y,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return np.sum(y - y_hat, axis=0)\n",
    "    return 1/2*np.sum(np.power(y - y_hat, 2),\\\n",
    "                     axis=0)\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):\n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN functions 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    A1 = (X @ W1 + b1)[0]\n",
    "    Z1 = logistic_sigmoid(A1) \n",
    "    A2 = (Z1 @ W2 + b2)[0]\n",
    "    Y = logistic_sigmoid(A2)/np.sum(A2,axis=1)\n",
    "    return A1, A2, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def backprop(W2, A1, A2, X, Y, t):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "\n",
    "    ##  2nd layer (hidden units)\n",
    "    # 1 x 2 or 4 x 2 (batch)\n",
    "    step1 = (Y - t).reshape() * logistic_sigmoid(A2, derivative=1)\n",
    "    step1 = step1.reshape(X.shape[0], X.shape[1])\n",
    "    \n",
    "    # 1 X N hidden units\n",
    "    step2 = logistic_sigmoid(A1, derivative=1)\n",
    "    step2 = step2.reshape(1, step2.shape[0])\n",
    "\n",
    "     # N x 1\n",
    "    step3 = W2\n",
    "    \n",
    "    grad_mid_layer = step1.T @ step2 @ step3 @ X.T\n",
    "    \n",
    "    ##  output layer (first step has been calculated already)    \n",
    "    # now it's 2 x 1\n",
    "    step1 = step1.T\n",
    "    # 1 X N hidden units\n",
    "    \n",
    "    step2 = logistic_sigmoid(A1, derivative=0)\n",
    "    N_no_hid_units = step2.shape[0]\n",
    "    \n",
    "    step2 = step2.reshape(1, N_no_hid_units)\n",
    "    \n",
    "    grad_output = step1 @ step2\n",
    "    \n",
    "    return grad_mid_layer, grad_output\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>\n",
    "def init_weights_biases(NO_UNITS_L1):\n",
    "    np.random.seed(1) #shown to converge for other XOR regression problem\n",
    "    \n",
    "    W1 = np.random.randn(2, NO_UNITS_L1)\n",
    "    b1 = np.zeros((1, NO_UNITS_L1))\n",
    "    W2 = np.random.randn(NO_UNITS_L1, 2) # 2 outputs, P(0) and P(1)\n",
    "    b2 = np.zeros((1,2))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#online (sample by sample) training\n",
    "# all samples (X), 4 x 2, are fed\n",
    "def train(X, T, NO_UNITS_L1=4, epochs=10_000,\\\n",
    "          learning_rate=.1, show_cost=0):\n",
    "    \n",
    "    converged = False\n",
    "    rho = learning_rate\n",
    "    Y = np.zeros((X.shape[0], 2)) # 4 x 2 always (the whole dataset is fed)\n",
    "    \n",
    "    W1, b1, W2, b2 = init_weights_biases(NO_UNITS_L1)\n",
    "    \n",
    "    if show_cost:\n",
    "        print()\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        idx_done = i + 1\n",
    "        for j in range(X.shape[0]):\n",
    "            A1, A2, Y[j,:] = forward_prop(W1, b1, W2, b2, X[j,:].reshape(1,2))\n",
    "            \n",
    "            grad_mid_layer, grad_output =\\\n",
    "                    backprop(W2, A1, A2, X[j,:].reshape(1,2),\\\n",
    "                             Y[j,:].reshape(1,2), T[j,:].reshape(1,2))\n",
    "            \n",
    "            W1 = W1 - rho*grad_mid_layer\n",
    "            W2 = W2 - rho*grad_output.T\n",
    "#             b1 = b1 - rho*np.mean(grad_mid_layer)\n",
    "#             b2 = b2 - rho*np.mean(grad_output)\n",
    "            \n",
    "            if show_cost:\n",
    "                print(\"cost: \" + str(cost(T[j,:],Y)))\n",
    "        \n",
    "        probabilities = np.array(np.round(Y) == T)\n",
    "        \n",
    "        if len(probabilities[probabilities == False]) < 1: #converged\n",
    "            converged = True\n",
    "            break\n",
    "            \n",
    "    return [W1, b1, W2, b2, X, Y, idx_done, epochs, converged, rho]\n",
    "\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#train package is a list with [W1, b1, W2, b2, X]\n",
    "# n: 0-3 selection of logical inputs; e.g. 0 == [0, 0]; 3 == [1,1]\n",
    "def predict(train_pkg, n):\n",
    "    A1, A2, Y = forward_prop(train_pkg[0], train_pkg[1],\\\n",
    "                             train_pkg[2], train_pkg[3], \\\n",
    "                             train_pkg[4][n,:])\n",
    "    del A1, A2\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN functions 2 (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24499999, 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, no_hidden_units=2,\\\n",
    "                    iterations=500, rho=.01, print_cost=0):\n",
    "    train_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # NO_UNITS_L1 = 6  yields max matches with rho = 1 and epochs = 500\n",
    "#         train_gates[i] = train(X, t[i], NO_UNITS_L1=2, epochs=500, learning_rate=1)\n",
    "        train_gates[i] = train(X, t[i], NO_UNITS_L1=no_hidden_units,\\\n",
    "                               epochs=iterations, learning_rate=rho,\\\n",
    "                               show_cost=print_cost)\n",
    "    return train_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# train_pkg_all_gates: 0, 1, 2, 3, 4, 5  [W1, b1, W2, b2, X, Y, \n",
    "            #                                 6. idx_done, \n",
    "            #                                 7. epochs\n",
    "            #                                 8. converged\n",
    "            #                                 9. rho\n",
    "def match_all_gate_outputs(train_pkg_all_gates, t):\n",
    "    \n",
    "    test = range(0,4)\n",
    "    \n",
    "    matches_dummy = np.zeros((4,len(t)))\n",
    "    matches = {}\n",
    "    list_dummy = []\n",
    "    \n",
    "    j = 0\n",
    "    for i in t:\n",
    "        for k in test:\n",
    "            y = predict(train_pkg_all_gates[i], k)\n",
    "            a = np.round(y) == t[i][k,:] # t[i][k,:] is the target\n",
    "            a = (a[0] == True) and (a[1] == True)\n",
    "            matches_dummy[k, j] = a\n",
    "        # for train_pkg_all_gates:\n",
    "        #                                 8. converged\n",
    "        #                                 6. idx_done, \n",
    "        #                                 7. epochs\n",
    "        #                                 9. rho\n",
    "        list_dummy = [train_pkg_all_gates[i][8], train_pkg_all_gates[i][6],\\\n",
    "                      train_pkg_all_gates[i][7], train_pkg_all_gates[i][9],\\\n",
    "                      matches_dummy[:, j]]\n",
    "        matches[i] = list_dummy\n",
    "        j += 1\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset / targets\n",
    "X: possible inputs of a logic function.\n",
    "t: dictionary with possible outputs for each logic gates. \n",
    "    4 binary ouputs to match NN's output probabilities of 0 or 1. \n",
    "    - if [p(0) p(1)] == [1 0] then probability of 0 == 1 && probability of 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NAND\": np.array([[0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"OR\": np.array([[1, 0],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NOR\": np.array([[0, 1],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"XOR\": np.array([[1, 0],\\\n",
    "                     [0, 1],\\\n",
    "                     [0, 1],\\\n",
    "                     [1, 0]], dtype=np.float32) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gates = train_all_gates(X, t, no_hidden_units=8,\\\n",
    "                              iterations=500, rho=4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = match_all_gate_outputs(train_gates, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches\n",
    "matches is a Python dictionary. for ex: \n",
    "\n",
    "    matches[\"OR\"]\n",
    "    returns a list:\n",
    "            matches[\"OR\"][0] == boolean (converged or not, True or False)\n",
    "            matches[\"OR\"][1] == idx_done, no. of iterations to converge\n",
    "            matches[\"OR\"][2] == total iterations (epochs)\n",
    "            matches[\"OR\"][3] == learning rate used(rho)\n",
    "            matches[\"OR\"][4] == matches with target (comparing poth p(0) and p(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in matches:\n",
    "    print(i + \": \" + str(matches[i][0]))\n",
    "    print(\"iterations to converge: \" + str(matches[i][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
