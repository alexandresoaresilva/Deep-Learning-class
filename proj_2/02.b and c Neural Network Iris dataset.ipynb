{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        Z.insert(i, W[i] @ A[i] + B[i])\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    Y = np.divide(A[i+1], np.sum(A[i+1], axis=0)) #comuns are the samples now\n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = ( cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1))\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "#         dJ_dW.insert(i+1, error[i+1] @ A[i+1].T)\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "#         error.insert(i, error_dummy)\n",
    "        error[i] = error_dummy\n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    \n",
    "    return dJ_dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "def train(X, T, hidden_layers=[2], epochs=500, rho=.1, normalize_data=True, show_cost=0):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features, no_outputs, hidden_layers)\n",
    "    \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))\n",
    "    \n",
    "    j = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "        \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW = backprop(W, Z, A, Y_hat, T.T)\n",
    "\n",
    "        #grad descent\n",
    "        for j in range(len(W)):\n",
    "            W[j] = W[j] - rho*dJ_dW[j]\n",
    "\n",
    "        Y = Y_hat.T\n",
    "\n",
    "        y_and_T_match = np.allclose(Y, T, rtol=1e-03)        \n",
    "\n",
    "        if y_and_T_match: #converged\n",
    "            j += 1 \n",
    "            if j == 3:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            if j > 100: #makes the prediction more robust \n",
    "                # ( probability considered 1 == .60 or greater )\n",
    "                converged = True\n",
    "                break\n",
    "    \n",
    "        cost_final.append(cost_MSE(T, Y_hat.T))\n",
    "        accuracy.append(calc_accuracy(T, Y_hat.T))\n",
    "        \n",
    "        if show_cost:\n",
    "            print(str(i) + \": accur: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\" cost: \" + str(cost_final[i]))\n",
    "    \n",
    "    if show_cost:\n",
    "        display_NN_info()\n",
    "        plt.scatter(range(epochs), cost_final, s=1, color=\"red\")\n",
    "        plt.title(\"iterations X cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"cost\")\n",
    "    \n",
    "    \n",
    "    print(\"   Start/final Cost: \" + \"{:.6f}\".format(cost_final[0]) \\\n",
    "          + \"/\" + \"{:.6f}\".format(cost_final[-1]))\n",
    "    \n",
    "    print(\"   Train start/final accuracy(\" + str(no_samples) + \" train samples): \"\\\n",
    "          + \"{:.2f}\".format(accuracy[0]) + \"/\" + \"{:.2f}\".format(accuracy[-1]) )\n",
    "    \n",
    "    return W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho]\n",
    "def predict(X, T, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "    \n",
    "    \n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset 2.b and 2.c Iris - includes function to select test and train sets\n",
    "\n",
    "    T[0:50,0]: setosa\n",
    "    T[50:100,1]: versicolor\n",
    "    T[100:150,2]:virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel (r'fisheriris.xlsx')\n",
    "names = [\"setosa\", \"versicolor\", \"virginica\"]\n",
    "idx = [50,100,150]\n",
    "\n",
    "# first four columns are \n",
    "X = np.array(df.values[:,0:4], dtype=np.float32)\n",
    "T = np.zeros((150,3))\n",
    "T[0:50,0] = 1 # setosa\n",
    "T[50:100,1] = 1 #versicolor\n",
    "T[100:150,2] = 1 #virginica\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> select_train_test_samples >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def select_train_test_samples(X, T, no_samples_train):\n",
    "    size = X.shape[0]\n",
    "\n",
    "    last_index = size-1\n",
    "    \n",
    "    all_indeces = np.linspace(0, last_index, size, dtype=np.int16)\n",
    "    \n",
    "    samples = []\n",
    "    labels = []\n",
    "    idx = []\n",
    "    random.seed(5)\n",
    "    idx.append(random.sample(range(size), no_samples_train))\n",
    "    idx.append(np.delete(all_indeces, idx)) # delete returns a different value every time\n",
    "    \n",
    "    for i in range(2):\n",
    "        samples.insert(i, X[idx[i],:]) # samples[0] is train, [1] are test\n",
    "        labels.insert(i, T[idx[i],:]) # same for labels\n",
    "    \n",
    "    return [samples[0], samples[1], labels[0], labels[1]]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> train_random_samples >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_random_samples(X,T, no_of_train_samples, hidden_layers=[2],\\\n",
    "                         epochs=1000, rho=.1, normalize_data=True, show_cost=0):\n",
    "    \n",
    "    no_test_samples = X.shape[0]-no_of_train_samples\n",
    "    \n",
    "    print(\"train/test samples: \" + str(no_of_train_samples) + \"/\" + str(no_test_samples) )\n",
    "    [X_train, X_test, T_train,T_test] = select_train_test_samples(X, T, no_of_train_samples)\n",
    "    \n",
    "    start = time.time()\n",
    "    train_pkg = train(X_train, T_train, hidden_layers=hidden_layers, epochs=epochs, rho=rho, show_cost=show_cost)\n",
    "    end = time.time()\n",
    "    Y_hat, accuracy = predict(X_test, T_test, train_pkg)\n",
    "    \n",
    "    print(\"   Test accuracy(\" + str(no_test_samples) +\" test samples): \" + \"{:.2f}\".format(accuracy) + \"%\")\n",
    "    print('   Time Taken: ', end-start, ' seconds')\n",
    "    print()\n",
    "    \n",
    "    return accuracy, train_pkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN run (ONE HIDDEN LAYER)\n",
    "\n",
    "The Neural Network can be run in two ways (\n",
    "\n",
    "    1. using train_random_samples() with parameters will train based on a slice of the data for the train; the other part will be used for test\n",
    "    2. using train() and then feeding the train package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test samples: 50/100\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 3\n",
      "   learnable weights: 14\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 6.102624/0.330640\n",
      "   Train start/final accuracy(50 train samples): 0.00/100.00\n",
      "   Test accuracy(100 test samples): 88.00%\n",
      "   Time Taken:  4.853496551513672  seconds\n",
      "\n",
      "train/test samples: 75/75\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 3\n",
      "   learnable weights: 14\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 9.147789/0.630011\n",
      "   Train start/final accuracy(75 train samples): 0.00/97.33\n",
      "   Test accuracy(75 test samples): 96.00%\n",
      "   Time Taken:  4.877950191497803  seconds\n",
      "\n",
      "train/test samples: 100/50\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 3\n",
      "   learnable weights: 14\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 12.405826/0.767618\n",
      "   Train start/final accuracy(100 train samples): 0.00/97.00\n",
      "   Test accuracy(50 test samples): 92.00%\n",
      "   Time Taken:  4.8669798374176025  seconds\n",
      "\n",
      "train/test samples: 125/25\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 3\n",
      "   learnable weights: 14\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 15.329107/0.911150\n",
      "   Train start/final accuracy(125 train samples): 0.80/97.60\n",
      "   Test accuracy(25 test samples): 96.00%\n",
      "   Time Taken:  4.938829660415649  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_train_samples = [50, 75, 100, 125]\n",
    "accuracys = {}  #dictionary indexed with number of train samples used\n",
    "train_pkgs = {} #same as previous\n",
    "\n",
    "\n",
    "for i in range(len(no_train_samples)):\n",
    "    accuracys[no_train_samples[i]], train_pkgs[no_train_samples[i]] = \\\n",
    "                            train_random_samples(X,T, no_train_samples[i], hidden_layers=[2],\\\n",
    "                                     epochs=5_000, rho=.1, normalize_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c. NN run (TWO LAYERS-DEEP)\n",
    "\n",
    "The Neural Network can be run in two ways (\n",
    "\n",
    "    1. using train_random_samples() with parameters will train based on a slice of the data for the train; the other part will be used for test\n",
    "    2. using train() and then feeding the train package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test samples: 50/100\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 1 units\n",
      "   layer 3: 3 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 16\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 5.734385/0.038773\n",
      "   Train start/final accuracy(50 train samples): 28.00/100.00\n",
      "   Test accuracy(100 test samples): 94.00%\n",
      "   Time Taken:  4.899377822875977  seconds\n",
      "\n",
      "train/test samples: 75/75\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 1 units\n",
      "   layer 3: 3 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 16\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 8.526915/0.426495\n",
      "   Train start/final accuracy(75 train samples): 29.33/98.67\n",
      "   Test accuracy(75 test samples): 94.67%\n",
      "   Time Taken:  4.935793161392212  seconds\n",
      "\n",
      "train/test samples: 100/50\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 1 units\n",
      "   layer 3: 3 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 16\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 11.084268/3.583296\n",
      "   Train start/final accuracy(100 train samples): 35.00/69.00\n",
      "   Test accuracy(50 test samples): 62.00%\n",
      "   Time Taken:  5.050464630126953  seconds\n",
      "\n",
      "train/test samples: 125/25\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 4\n",
      "   layer 2: 1 units\n",
      "   layer 3: 3 units\n",
      "   output layer (4): 3\n",
      "   learnable weights: 16\n",
      "   max epochs: 5,000\n",
      "   learning rate(rho): 0.1\n",
      "   Start/final Cost: 13.930431/0.570268\n",
      "   Train start/final accuracy(125 train samples): 33.60/99.20\n",
      "   Test accuracy(25 test samples): 96.00%\n",
      "   Time Taken:  5.158007383346558  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_train_samples = [50, 75, 100, 125]\n",
    "accuracys = {}  #dictionary indexed with number of train samples used\n",
    "train_pkgs = {} #same as previous\n",
    "\n",
    "for i in range(len(no_train_samples)):\n",
    "    accuracys[no_train_samples[i]], train_pkgs[no_train_samples[i]] = \\\n",
    "                            train_random_samples(X,T, no_train_samples[i], hidden_layers=[1,3],\\\n",
    "                                     epochs=5_000, rho=.1, normalize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
