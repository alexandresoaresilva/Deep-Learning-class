{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y,y_hat):\n",
    "    return 1/2*np.sum( np.power(y - y_hat, 2) )\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):\n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    \n",
    "    return sigm\n",
    "\n",
    "# from https://deepnotes.io/softmax-crossentropy\n",
    "def stable_softmax(a, derivative=0):\n",
    "    exps = np.exp(a - np.max(a))\n",
    "\n",
    "    if len(exps.shape) > 1:\n",
    "        ans = np.zeros((exps.shape[0],2))\n",
    "        numerator = exps.sum(axis=1).reshape((exps.shape[0], 1))\n",
    "        numerator = np.append(numerator, numerator, axis=1)\n",
    "    else:\n",
    "#         ans = np.zeros((exps.shape[0],))\n",
    "        ans = np.zeros((2,2))\n",
    "        numerator = np.sum(exps)\n",
    "        \n",
    "    S = exps/numerator\n",
    "    \n",
    "    if derivative:\n",
    "        if len(exps.shape) > 1: # for more than one sample\n",
    "            for i in range(exps.shape[0]):\n",
    "                ans[i, 0] = S[i, 1]*(1 - S[i, 0])\n",
    "                ans[i, 1] = S[i, 0]*(1 - S[i, 1])\n",
    "        else:\n",
    "            kro_delta = 0\n",
    "            for i in range(ans.shape[0]):\n",
    "                for j in range(ans.shape[1]):                        \n",
    "                    if i==j:\n",
    "                        kro_delta = 1\n",
    "                    else:\n",
    "                        kro_delta = 0\n",
    "                        \n",
    "                    ans[i,j] = S[i]*(kro_delta - S[j])\n",
    "        return ans\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    A1 = (X @ W1 + b1)[0]\n",
    "    Z1 = logistic_sigmoid(A1) \n",
    "    A2 = (Z1 @ W2 + b2)[0]\n",
    "    Y = stable_softmax(A2)\n",
    "    return A1, A2, Y\n",
    "\n",
    "def backprop(W2, A1, A2, X, Y, t):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0])\n",
    "\n",
    "## >>>>>>>>>>>>>>>>>> 2nd layer (hidden units)\n",
    "    # 1 x 2 or 4 x 2 (batch)\n",
    "    step1 = (Y - t) @ stable_softmax(A2, derivative=1)\n",
    "    step1 = step1.reshape(X.shape[0], X.shape[1])\n",
    "    \n",
    "    # 1 X N hidden units\n",
    "    step2 = logistic_sigmoid(A1, derivative=1)\n",
    "    step2 = step2.reshape(1, step2.shape[0])\n",
    "\n",
    "     # N x 1\n",
    "    step3 = W2\n",
    "    \n",
    "    grad_mid_layer = step1.T @ step2 @ step3 @ X.T\n",
    "    \n",
    "## >>>>>>>>>>>>>>>>>> output layer (first step has been calculated already)    \n",
    "    # now it's 2 x 1\n",
    "    step1 = step1.T\n",
    "    # 1 X N hidden units\n",
    "    \n",
    "    step2 = logistic_sigmoid(A1, derivative=0)\n",
    "    N_no_hid_units = step2.shape[0]\n",
    "    \n",
    "    step2 = step2.reshape(1, N_no_hid_units)\n",
    "    \n",
    "    grad_output = step1 @ step2\n",
    "    \n",
    "    return grad_mid_layer, grad_output\n",
    "\n",
    "#online (sample by sample) training\n",
    "def train(X, T, NO_UNITS_L1=4, epochs=10_000, learning_rate=.1, show_cost=0):\n",
    "    \n",
    "    rho = learning_rate\n",
    "    W1 = np.random.randn(2, NO_UNITS_L1)\n",
    "    b1 = np.zeros((1, NO_UNITS_L1))\n",
    "    W2 = np.random.randn(NO_UNITS_L1, 2) # 2 outputs, P(0) and P(1)\n",
    "    b2 = np.zeros((1,2))\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for j in range(X.shape[0]):\n",
    "            A1, A2, Y = forward_prop(W1, b1, W2, b2, X[j,:])\n",
    "            grad_mid_layer, grad_output = backprop(W2, A1, A2, X[j,:], Y, T[j,:])\n",
    "            W1 = W1 - rho*grad_mid_layer\n",
    "            W2 = W2 - rho*grad_output.T\n",
    "            b1 = b1 - rho*np.mean(grad_mid_layer)\n",
    "            b2 = b2 - rho*np.mean(grad_output)\n",
    "            if show_cost:\n",
    "                print(\"cost: \" + str(cost(T[j,:],Y)))\n",
    "    \n",
    "    return [W1, b1, W2, b2, X]\n",
    "\n",
    "#train package is a list with [W1, b1, W2, b2, X]\n",
    "def predict(train_pkg, n):\n",
    "    A1, A2, Y = forward_prop(train_pkg[0], train_pkg[1],\\\n",
    "                             train_pkg[2], train_pkg[3], \\\n",
    "                             train_pkg[4][n,:])\n",
    "    \n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset / targets\n",
    "X: possible inputs of a logic function.\n",
    "t: dictionary with possible outputs for each logic gates. \n",
    "    4 binary ouputs to match NN's output probabilities of 0 or 1. \n",
    "    - if [p(0) p(1)] == [1 0] then probability of 0 == 1 && probability of 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NAND\": np.array([[0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"OR\": np.array([[1, 0],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NOR\": np.array([[0, 1],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"XOR\": np.array([[1, 0],\\\n",
    "                     [0, 1],\\\n",
    "                     [0, 1],\\\n",
    "                     [1, 0]], dtype=np.float32) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_gates = {}\n",
    "\n",
    "for i in t:\n",
    "    train_gates[i] = train(X, t[i], NO_UNITS_L1=8, epochs=10_000)\n",
    "#     train(X, T, NO_UNITS_L1=4, epochs=10_000, learning_rate=.1, show_cost=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND\n",
      "t: [0. 1.]\n",
      "Y: [0.04395246 0.95604754]\n",
      "NAND\n",
      "t: [1. 0.]\n",
      "Y: [0.95092582 0.04907418]\n",
      "OR\n",
      "t: [0. 1.]\n",
      "Y: [1.69920905e-04 9.99830079e-01]\n",
      "NOR\n",
      "t: [1. 0.]\n",
      "Y: [9.99738297e-01 2.61703058e-04]\n",
      "XOR\n",
      "t: [1. 0.]\n",
      "Y: [0.79582512 0.20417488]\n"
     ]
    }
   ],
   "source": [
    "test = 3 # select the binary inputs (test = 0 = > [0,0]; test = 3 = > [1,1])\n",
    "# gate = \"NAND\"\n",
    "for i in t:\n",
    "    y = predict(train_gates[i], test)\n",
    "    print(i)\n",
    "    print(\"t: \" + str(t[i][test,:]))\n",
    "    print(\"Y: \" + str(y) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
