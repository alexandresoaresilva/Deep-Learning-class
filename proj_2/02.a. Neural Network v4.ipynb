{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "# from numba import vectorize\n",
    "# from numba import cuda\n",
    "from numba import autojit, jit\n",
    "# import cupy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost/activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cuda.jit\n",
    "def cost_MSE(t,y_hat, derivative=0):\n",
    "    if derivative:\n",
    "            return -(t - y_hat)\n",
    "    return np.mean(1/2*np.sum(np.power(t - y_hat, 2),\\\n",
    "                      axis=0))\n",
    "# @cuda.jit(device=True)\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "\n",
    "def logistic_sigmoid(x, derivative=0):    \n",
    "    sigm = 1/(1 + np.exp(-x))\n",
    "    if len(sigm.shape) < 2:\n",
    "        sigm = sigm.reshape(sigm.shape[0],1)\n",
    "        \n",
    "    if derivative:\n",
    "        return sigm*(1. - sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> init_weights_biases >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W, B = init_weights_biases(4, 3, [2,2])\n",
    "#no_hidden_units: needs a list with at least one element\n",
    "# @cuda.jit(device=True)\n",
    "def init_weights_biases(no_of_features, no_outputs, no_hidden_units, seed=1):\n",
    "    \n",
    "    W = []\n",
    "    B = []\n",
    "    rows, columns = 0, 0 \n",
    "    last = len(no_hidden_units)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if no_hidden_units: #list is not empty\n",
    "        for i in range(last+1):\n",
    "            if i == 0: #first weight\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_of_features\n",
    "            elif i > 0 and i < last:\n",
    "                rows = no_hidden_units[i]\n",
    "                columns = no_hidden_units[i-1]\n",
    "            else: #last\n",
    "                columns = rows # list ran out of indeces, so use last one\n",
    "                rows = no_outputs            \n",
    "\n",
    "            W.insert(i, np.random.randn(rows, columns))\n",
    "            B.insert(i, np.zeros((rows, 1)))\n",
    "    else: # no hidden units (perceptron)\n",
    "        W.insert(0, np.random.randn(no_outputs, no_of_features))\n",
    "        B.insert(0, np.zeros((no_outputs, 1)))\n",
    "    \n",
    "    dummy_param = 0\n",
    "    param = 0\n",
    "    for i in range(len(W)):\n",
    "        dummy_param = W[i].shape[0] * W[i].shape[1]\n",
    "        param += dummy_param\n",
    "        \n",
    "#     W.append(param) #number of learnable weights\n",
    "    \n",
    "    return W, B, param\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> forward_prop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "    # Z, A, Y = forward_prop(W, B, X)\n",
    "    # X has n features x M samples\n",
    "# @autojit\n",
    "# @cuda.jit\n",
    "# @jit(nopython=True)\n",
    "def forward_prop(W, B, X):\n",
    "    no_of_samples = X.shape[1]\n",
    "     #last weight matrix, rows correspond to outputs\n",
    "    no_of_outputs = W[-2].shape[0] #index -1 is the number of learnable weights\n",
    "    \n",
    "    Z = []\n",
    "    A = []\n",
    "    A.append(X) #first layer is an activation\n",
    "    \n",
    "    for i in range(len(W)): #to avoid the last two indeces\n",
    "        bias = np.mean(B[i],axis=1)\n",
    "        bias = bias.reshape(bias.shape[0],1)\n",
    "        Z.insert(i, W[i] @ A[i] + bias)\n",
    "        A.insert(i+1, logistic_sigmoid(Z[i]))\n",
    "    \n",
    "    Y = np.zeros((no_of_samples, no_of_outputs))\n",
    "    #scaling to making the pair a probability\n",
    "    \n",
    "     #comuns are the samples now, so rows are features/outputs\n",
    "    if A[i+1].shape[0] > 1: #multiclass classification/normalized (all samples add up to 1)\n",
    "        Y = np.divide(A[i+1], np.sum(A[i+1], axis=0))\n",
    "    else:  #there's only one output per sample, so no normalization needed\n",
    "        Y = A[i+1]\n",
    "        \n",
    "    return Z, A, Y\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> backprop >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# W1, b1, W2, b2 = init_weights_biases(no_hidden_units=8)\n",
    "# A1, A2, Y = forward_prop(W1, b1, W2, b2, X)\n",
    "# grad_mid_layer, grad_output = backprop(W2, A1, A2, X, Y, t)\n",
    "# backprop(W2, A1, A2, X, Y, t)\n",
    "# @cuda.jit(device=True)\n",
    "\n",
    "# @cuda.jit\n",
    "# @autojit\n",
    "# @jit(nopython=True)\n",
    "def backprop(W, Z, A, Y_hat, T):\n",
    "    \n",
    "    output_index = len(W)-1 # if 3, starts at 2\n",
    "    error = {}\n",
    "    \n",
    "    error_output = cost_MSE(T,Y_hat, derivative=1) * logistic_sigmoid(Z[-1], derivative=1)\n",
    "    error[output_index] = error_output\n",
    "    \n",
    "    dJ_dW = {}\n",
    "    dJ_dB = {}\n",
    "    for i in range(output_index-1,-1,-1):\n",
    "         # doesn't get to W[0], so updated after the foor loop again\n",
    "        dJ_dW[i+1] = error[i+1] @ A[i+1].T\n",
    "        deriv_B = np.mean(logistic_sigmoid(Z[i], derivative=1),axis=1)\n",
    "        deriv_B = deriv_B.reshape(deriv_B.shape[0],1)\n",
    "        \n",
    "        dJ_dB[i+1] = dJ_dW[i+1] @ deriv_B #not sure if this is right\n",
    "        \n",
    "        error_dummy = (W[i+1].T @ error[i+1]) * logistic_sigmoid(Z[i], derivative=1)\n",
    "        error[i] = error_dummy\n",
    "        \n",
    "    \n",
    "    dJ_dW[0] = error[0] @ A[0].T\n",
    "    dJ_dB[0] = dJ_dW[0]\n",
    "    \n",
    "    return dJ_dW, dJ_dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN frontend 1 \n",
    "\n",
    "    train, predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # all samples (X), 4 x 2, are fed\n",
    "    # X: dataset, n samples x N features\n",
    "    # T: binary labels, n labels x L number of ouputs\n",
    "    # hidden_layers : list with number of neurons for each inner layer.\n",
    "        # e.g. [3, 4] will yield two layers with 3 and 4 units respectively\n",
    "    # this function needs n samples > 1 (batch optimization).\n",
    "# @cuda.jit(device=True)\n",
    "# @autojit\n",
    "def train(X, T, hidden_layers=[2], epochs=500,\\\n",
    "          rho=.1, normalize_data=False, show_cost=0, seed=1, w_transfer=[]):    \n",
    "    \n",
    "    if normalize_data:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    no_of_features = X.shape[1]\n",
    "    no_samples = X.shape[0]\n",
    "    no_outputs = T.shape[1]\n",
    "    \n",
    "    W, B, param = init_weights_biases(no_of_features,\\\n",
    "                                      no_outputs, hidden_layers, seed=1)\n",
    "    \n",
    "    if w_transfer:\n",
    "        W=w_transfer\n",
    "        \n",
    "    Y_hat = np.zeros((no_outputs, X.shape[0]))    \n",
    "\n",
    "    N = int(np.round(epochs/20))\n",
    "    i_for_show_cost = np.round(np.linspace(0,epochs,N))\n",
    "    \n",
    "    ###NESTED function\n",
    "    def display_NN_info():\n",
    "        print(\"* NN ************************************\")\n",
    "        print(\"   no. inputs (layer 1): \" + str(no_of_features) )\n",
    "        for k in range(len(hidden_layers)):\n",
    "            print(\"   layer \" + str(k+2) + \": \" + str(hidden_layers[k]) + \" units\")\n",
    "        print(\"   output layer (\"+ str(k+3) + \"): \" + str(no_outputs) )\n",
    "        print(\"   learnable weights: \" + str(param) )\n",
    "        print(\"   max epochs: \" + \"{:,}\".format(epochs) )\n",
    "        print(\"   learning rate(rho): \" + str(rho) )\n",
    "        \n",
    "    display_NN_info()\n",
    "    time.sleep(4)\n",
    "    #loop variables, along with other variables and lists (cost, accuracy)\n",
    "    # that are returned by the function\n",
    "    cost_final = []\n",
    "    accuracy = []\n",
    "    iter_passed = []\n",
    "    match = 0\n",
    "    idx_done = 0\n",
    "    converged = False\n",
    "    k_cost = 0\n",
    "    acc_count = 0\n",
    "    for i in range(epochs):\n",
    "        Z, A, Y_hat = forward_prop(W, B, X.T)\n",
    "        dJ_dW, dJ_dB = backprop(W, Z, A, Y_hat, T.T)\n",
    "    \n",
    "        #grad descent\n",
    "        for a in range(len(W)):\n",
    "            W[a] = W[a] - rho*dJ_dW[a]\n",
    "            B[a] = B[a] - rho*dJ_dB[a]\n",
    "        \n",
    "        if match <=5:\n",
    "            idx_done = i + 1\n",
    "        \n",
    "        if is_equal_in_boolean_terms(Y_hat.T, T):\n",
    "            match += 1            \n",
    "                \n",
    "            if match > 6 and match < 7:\n",
    "                idx_done = i + 1 # already predicts corretly all the time\n",
    "            \n",
    "        if match > 100: #seeing the match 100 times (for robustness)\n",
    "            converged = True #makes the predictions more confident\n",
    "            break        \n",
    "        \n",
    "        cost_final.insert(i,cost_MSE(T, Y_hat.T))\n",
    "        accuracy.insert(i,calc_accuracy(T, Y_hat.T))\n",
    "        iter_passed.insert(i,i)\n",
    "            \n",
    "        if show_cost and i_for_show_cost[k_cost] == i:\n",
    "            print(\"   \" + str(i) + \" iteration, accuracy: \"+ str(accuracy[i])+ \"%\")\n",
    "            print(\"   cost: \" + str(cost_final[i]))\n",
    "            k_cost += 1        \n",
    "        \n",
    "    if converged:\n",
    "        print(\"   Converged in \" + str(idx_done) + \" iterations\")\n",
    "    else:\n",
    "        print(\"   Did not converge.\")\n",
    "    print()\n",
    "    if show_cost:\n",
    "        plt.scatter(iter_passed, cost_final, s=1, color=\"red\")\n",
    "        plt.title(\"iterations X cost\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        \n",
    "    return [W, B, Y_hat.T, X, cost_final, epochs,\\\n",
    "            idx_done, converged, rho, normalize_data, accuracy]\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> predict >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    # X: dataset, n samples x N features\n",
    "    #  train_pkg: list with [W, B, Y, X, cost_final, epochs, idx_done, converged, rho, normalize_data, accuracy]\n",
    "def predict(X, T, train_pkg):\n",
    "    if len(X.shape) < 2:\n",
    "        X = X.reshape(1,X.shape[0]) #for one sample\n",
    "    \n",
    "    normalized = train_pkg[-1]\n",
    "    if normalized: #if the data has been normalized\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    Z, A, Y_hat = forward_prop(train_pkg[0], train_pkg[1], X.T)\n",
    "    \n",
    "    del Z, A\n",
    "    return Y_hat.T, calc_accuracy(T, Y_hat.T)\n",
    "\n",
    "def is_equal_in_boolean_terms(Y_hat, T):\n",
    "    return np.array_equal(np.round(Y_hat), T)\n",
    "\n",
    "def calc_accuracy(T, Y):\n",
    "    matches = np.argmax(Y, axis=1) == np.argmax(T, axis=1)\n",
    "    return len(matches[matches == True])/len(matches)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN functions 2 (helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>> train_all_gates >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def train_all_gates(X, t, hidden_layers=[3], iterations=500, rho=.01, normalize_data=False, print_cost=0):\n",
    "    train_pkg_all_gates = {} #init dictionary\n",
    "\n",
    "    for i in t:\n",
    "        # hidden_layers = list of number of units for each layer. Minimum 1 [1]\n",
    "                #train_gates[i] : [W, B, Y, X, cost_final, epochs, idx_done, \\\n",
    "                            #  converged, rho, normalize_data, accuracy]\n",
    "        train_pkg_all_gates[i] = train(X, t[i],\\\n",
    "                                       hidden_layers = hidden_layers, \\\n",
    "                               epochs=iterations, rho=rho, \\\n",
    "                                       normalize_data=normalize_data, \\\n",
    "                                       show_cost=print_cost, seed=1)\n",
    "\n",
    "    return train_pkg_all_gates\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_logic_gate >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_logic_gate(X, T, train_pkg):\n",
    "    #y_hat is returned transposed already from predict\n",
    "    Y_hat, accuracy = predict(X, T, train_pkg)\n",
    "    \n",
    "#   train_pkg : [W, B, Y, X, cost_final, epochs, idx_done,\\\n",
    "#                converged, rho, normalize_data, accuracy]\n",
    "    # indeces used, especially:\n",
    "    #                                 7. converged\n",
    "    #                                 6. idx_done, \n",
    "    #                                 5. epochs\n",
    "    #                                 8. rho\n",
    "    match_pkg = [train_pkg[7], train_pkg[6],\\\n",
    "                  train_pkg[5], train_pkg[8],\\\n",
    "                  is_equal_in_boolean_terms(Y_hat, T), Y_hat]\n",
    "    return match_pkg\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>> match_all_gate_outputs >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "def match_all_gate_outputs(X, T, train_pkg_all_gates):\n",
    "    matches = {}\n",
    "\n",
    "    for i in t:\n",
    "        matches[i] = match_logic_gate(X, T[i], train_pkg_all_gates[i])\n",
    "\n",
    "    return matches\n",
    "\n",
    "def print_match(match):\n",
    "    print(i + \" converged: \" + str(match[0]))\n",
    "    print(\"===========================================\")\n",
    "    if match[1]:\n",
    "        print(\"  iter. to converge: \" + str(match[1]))\n",
    "    else:\n",
    "        print(\"  reached max. iter\")\n",
    "    print(\"  iter. max: \" + str(match[2]))\n",
    "    if match[4]:\n",
    "        print(\"  ==== CORRECT prediction ==== \")\n",
    "    else:\n",
    "        print(\"  ==== INCORRECT prediction ==== \")\n",
    "    print(\"  predicted y (y_hat): \")\n",
    "    print(np.round(match[5]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset / targets\n",
    "\n",
    "X: possible inputs of a binary logic function.\n",
    "\n",
    "t: dictionary with possible outputs for each logic gates. \n",
    "    4 binary ouputs to match NN's output probabilities of 0 or 1. \n",
    "    - if [p(0) p(1)] == [1 0] then probability of 0 == 1 && probability of 1 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],\\\n",
    "              [0,1],\\\n",
    "              [1,0],\\\n",
    "              [1,1]], dtype=np.float32)\n",
    "\n",
    "\n",
    "t = { #dictionary for getting both the target logic values and the correlated string \n",
    "    # binary labels to represent the probabilities of 1 or 0 (first column is 0, 2nd 1)\n",
    "    \"AND\": np.array([[1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NAND\": np.array([[0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [0, 1],\\\n",
    "                      [1, 0]], dtype=np.float32),\n",
    "    \n",
    "    \"OR\": np.array([[1, 0],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1],\\\n",
    "                    [0, 1]], dtype=np.float32),\n",
    "    \n",
    "    \"NOR\": np.array([[0, 1],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0],\\\n",
    "                     [1, 0]], dtype=np.float32),\n",
    "    \n",
    "#     \"XOR\": np.array([[1, 0],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [1, 0]], dtype=np.float32) }\n",
    "\n",
    "#     \"XOR\": np.array([[1, 0],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [0, 1],\\\n",
    "#                      [1, 0]], dtype=np.float32) }\n",
    "    \"XOR\": np.array([[0],\\\n",
    "                     [1],\\\n",
    "                     [1],\\\n",
    "                     [0]], dtype=np.float32) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN run\n",
    "\n",
    "The Neural Network can be run with train() and then feeding the train_package to predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 8\n",
      "   max epochs: 30,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 25.0%\n",
      "   cost: 0.7842129300675083\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.2424682164931113\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.20787225102549922\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.1670414733920031\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.12373571225380213\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.10023855362711204\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.08773069857500199\n",
      "   Converged in 44 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 8\n",
      "   max epochs: 30,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 75.0%\n",
      "   cost: 0.32697532126912043\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.2435098366825215\n",
      "   40 iteration, accuracy: 75.0%\n",
      "   cost: 0.22451349193155523\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.2020738944823709\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.16625061709147426\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.13548401793026368\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.11746688714181938\n",
      "   140 iteration, accuracy: 100.0%\n",
      "   cost: 0.10616639476892809\n",
      "   Converged in 52 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 8\n",
      "   max epochs: 30,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 75.0%\n",
      "   cost: 0.497381007411035\n",
      "   20 iteration, accuracy: 75.0%\n",
      "   cost: 0.2572939329436879\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.11058298371745809\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.05256528412962824\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.03079252616525694\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.02068339020427205\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.015032348179628383\n",
      "   Converged in 39 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 2\n",
      "   learnable weights: 8\n",
      "   max epochs: 30,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 25.0%\n",
      "   cost: 0.6138072439255936\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.21461046473621914\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.06574102222568676\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.030658197017226947\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.018566794640672145\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.01287127535838638\n",
      "   Converged in 25 iterations\n",
      "\n",
      "* NN ************************************\n",
      "   no. inputs (layer 1): 2\n",
      "   layer 2: 2 units\n",
      "   output layer (3): 1\n",
      "   learnable weights: 6\n",
      "   max epochs: 30,000\n",
      "   learning rate(rho): 1\n",
      "   0 iteration, accuracy: 100.0%\n",
      "   cost: 0.6218856635242578\n",
      "   20 iteration, accuracy: 100.0%\n",
      "   cost: 0.4772440956066093\n",
      "   40 iteration, accuracy: 100.0%\n",
      "   cost: 0.42769858023554896\n",
      "   60 iteration, accuracy: 100.0%\n",
      "   cost: 0.38410105605809675\n",
      "   80 iteration, accuracy: 100.0%\n",
      "   cost: 0.3464260286183485\n",
      "   100 iteration, accuracy: 100.0%\n",
      "   cost: 0.3137890389100022\n",
      "   120 iteration, accuracy: 100.0%\n",
      "   cost: 0.2858433159602741\n",
      "   140 iteration, accuracy: 100.0%\n",
      "   cost: 0.2620548620606107\n",
      "   Converged in 47 iterations\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_pkg_all_gates = train_all_gates(X, t, hidden_layers=[2],\\\n",
    "                    iterations=30_000, normalize_data=True, rho=1, print_cost=1)\n",
    "\n",
    "# (X, t, hidden_layers=[3], iterations=500, rho=.01, print_cost=0)\n",
    "matches = match_all_gate_outputs(X, t, train_pkg_all_gates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### matches\n",
    "matches is a Python dictionary. For ex: \n",
    "\n",
    "    matches[\"OR\"]\n",
    "    returns a list:\n",
    "            matches[\"OR\"][0] == boolean (converged or not, True or False)\n",
    "            matches[\"OR\"][1] == idx_done, no. of iterations to converge\n",
    "            matches[\"OR\"][2] == total iterations (epochs)\n",
    "            matches[\"OR\"][3] == learning rate used(rho)\n",
    "            matches[\"OR\"][4] == matches with target (comparing both p(0) and p(1))\n",
    "            matches[\"OR\"][5] == predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 44\n",
      "  iter. max: 30000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "NAND converged: True\n",
      "===========================================\n",
      "  iter. to converge: 52\n",
      "  iter. max: 30000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "\n",
      "OR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 39\n",
      "  iter. max: 30000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "\n",
      "NOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 25\n",
      "  iter. max: 30000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "\n",
      "XOR converged: True\n",
      "===========================================\n",
      "  iter. to converge: 47\n",
      "  iter. max: 30000\n",
      "  ==== CORRECT prediction ==== \n",
      "  predicted y (y_hat): \n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in matches:\n",
    "    print_match(matches[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, 47, 30000, 1, True, array([[0.32926094],\n",
       "        [0.57341142],\n",
       "        [0.65220838],\n",
       "        [0.3292607 ]])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
