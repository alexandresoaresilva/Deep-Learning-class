{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import random\n",
    "## keras stuff\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Input, Activation, \\\n",
    "                    Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution2D,Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.datasets import cifar10\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from keras.layers.core import Lambda\n",
    "import os\n",
    "\n",
    "os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting CIFAR-10 dataset\n",
    "    helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'br') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "#returns a uint8 numpy array with m x n x color(R,G,o B)\n",
    "def from_array_to_RGB(I_lin,channel_size=1024):\n",
    "    L = channel_size\n",
    "    side = int(np.sqrt(L)) #square image\n",
    "    \n",
    "    I = np.zeros((side,side,3),dtype=np.uint8)\n",
    "    #==== color channels\n",
    "    #R\n",
    "    I[:,:,0] = I_lin[0:L].reshape((side,side)) \n",
    "    #G\n",
    "    I[:,:,1] = I_lin[L:2*L].reshape((side,side))\n",
    "    #B\n",
    "    I[:,:,2] = I_lin[2*L:3*L].reshape((side,side))\n",
    "    return I\n",
    "\n",
    "## select_random_images_in_batch\n",
    "#     Selects random images from the loaded batch, and saves them in a list.\n",
    "#         1st column: image\n",
    "#         2nd column: label\n",
    "def select_random_images_in_batch(batch,no_of_samples=5):\n",
    "    selection = random.sample(range(batch[b'data'].shape[0]), no_of_samples)\n",
    "    I = []\n",
    "    \n",
    "    for i in range(len(selection)):\n",
    "        index = selection[i]\n",
    "        I.append( (from_array_to_RGB(batch[b'data'][index], \\\n",
    "                                   batch[b'data'][index].size//3),\\\n",
    "                                   batch[b'labels'][index], index) )\n",
    "               \n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display 25 random images from CIFAR dataset\n",
    "    \n",
    "       this selects 5 random images from each batch_n file, n = 1..5\n",
    "       \n",
    "       batches is a Python list that holds dictionaries:\n",
    "            batches[0][b'data] returns 10,000 horizontalized images.\n",
    "            batches[0][b'data][0] is the 1st image in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "path = os.getcwd() + \"\\\\cifar-10-batches-py\\\\\"\n",
    "file = []\n",
    "batches = []\n",
    "\n",
    "# file = ['data_batch_{}'.format(i) for i in range(5)]\n",
    "for i in range(5): #getting path + name of the folder for the dataset\n",
    "    batches.append(unpickle(path + \"data_batch_\" + str(i+1)))\n",
    "batches.append(unpickle( path + \"test_batch\" )) \n",
    "\n",
    "m = 5\n",
    "no_of_samples = m**2\n",
    "I_list = []\n",
    "for i in range(m):\n",
    "    I_list = select_random_images_in_batch(batches[i], no_of_samples)\n",
    "    for j in range(no_of_samples):\n",
    "        plt.subplot(m,m,j+1)\n",
    "        plt.imshow(I_list[j][0])\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing dataset\n",
    "    putting all batches together in one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60689121f0fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10_000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batches' is not defined"
     ]
    }
   ],
   "source": [
    "num_of_classes = 10\n",
    "\n",
    "X_train, X_test = [],[]\n",
    "t_train, t_test = [],[]\n",
    "\n",
    "k = 0\n",
    "for i in range(len(batches)):\n",
    "    for j in range(10_000):\n",
    "        if i == (len(batches)-1): #test set\n",
    "            X_test.append(from_array_to_RGB(batches[i][b'data'][j], \\\n",
    "                               batches[i][b'data'][j].size//3))\n",
    "            t_test.append(batches[i][b'labels'][j])        \n",
    "        else: #train test\n",
    "            X_train.append(from_array_to_RGB(batches[i][b'data'][j], \\\n",
    "                               batches[i][b'data'][j].size//3))\n",
    "            t_train.append(batches[i][b'labels'][j])\n",
    "   \n",
    "#another option is to download the dataset with this line of code\n",
    "# (x_train, y_train), (x_test, y_test) = cifar10.load_data() \n",
    "\n",
    "#binarize labels for use with VGG16\n",
    "t_train = keras.utils.np_utils.to_categorical(t_train, num_of_classes)\n",
    "t_test = keras.utils.np_utils.to_categorical(t_test, num_of_classes)\n",
    "\n",
    "#normalized by dividing byt the max\n",
    "X_train = np.asarray(X_train,dtype=np.float32)/255\n",
    "X_train = X_train.astype('float16')\n",
    "\n",
    "X_test = np.asarray(X_test,dtype=np.float32)/255\n",
    "X_test = X_test.astype('float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculations for stride/filter size and image size match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = lambda W,F,P,S: (W-F+2*P)/S + 1 \n",
    "#1. cond2d layer\n",
    "print(32)\n",
    "NEW_W = stride(32,4,0,2)\n",
    "# print(NEW_W)\n",
    "#1. maxpooling layer\n",
    "NEW_W = stride(NEW_W,3,0,1)\n",
    "print(NEW_W)\n",
    "#####################\n",
    "#2. cond2d layer\n",
    "NEW_W = stride(NEW_W,3,1,1)\n",
    "# print(NEW_W)\n",
    "#2. maxpooling layer\n",
    "NEW_W = stride(NEW_W,3,0,1)\n",
    "print(NEW_W)\n",
    "#####################\n",
    "#3. cond2d layer\n",
    "NEW_W = stride(NEW_W,3,1,1)\n",
    "# print(NEW_W)\n",
    "#3. maxpooling layer\n",
    "NEW_W = stride(NEW_W,3,0,1)\n",
    "print(NEW_W)\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "num_classes = 10\n",
    "X_shape = [32,32,3]\n",
    "\n",
    "learning_rate = 0.1\n",
    "lr_decay = 1e-6\n",
    "lr_drop = 20\n",
    "maxepochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple AlexNet - (very small) deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "def alexsoaresnet(no_outputs=10,weights_path=None):\n",
    "    weight_decay = 1e-6\n",
    "    model = Sequential()\n",
    "####################\n",
    "#     model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(48, (4, 4), strides=2, padding='valid', \\\n",
    "                     input_shape=(32,32,3), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "####################\n",
    "    model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(192, (3, 3), padding='valid', \\\n",
    "                     input_shape=(13,13,24), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=1))\n",
    "####################\n",
    "    model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), padding='valid', \\\n",
    "                     input_shape=(11,11,96), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=1))\n",
    "####################\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "####################\n",
    "    model.add(Dense(no_outputs))\n",
    "    model.add(Activation('softmax'))    \n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\\\n",
    "                  optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "\n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## image settings for running the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(    \n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "# (std, mean, and principal components if ZCA whitening is applied)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling and running the network\n",
    "    commented out by default because it takes long to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_net = alexsoaresnet()\n",
    "# as_net.summary()\n",
    "\n",
    "# alexSnet = as_net.fit_generator(datagen.flow(X_train, t_train,\\\n",
    "#                                         batch_size=batch_size),\\\n",
    "#                            steps_per_epoch=X_train.shape[0] // batch_size,\\\n",
    "#                            epochs=maxepochs, validation_data=(X_test, t_test),\\\n",
    "#                            callbacks=[reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## saving weights and plotting trainning stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_net.save_weights('transfer_cifar10alexSnet83.h5')\n",
    "# as_net.save('alexSnet.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del model  # deletes the existing model\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(alexSnet.history['acc'],'r')\n",
    "plt.plot(alexSnet.history['val_acc'],'g')\n",
    "plt.xticks(np.arange(0, 51, 2.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    " \n",
    "plt.figure(1)\n",
    "plt.plot(alexSnet.history['loss'],'r')\n",
    "plt.plot(alexSnet.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, 51, 2.0))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "scores = as_net.evaluate(X_test, t_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading merch dataset\n",
    "    labels are:\n",
    "    1. \"Cap\"\n",
    "    2. \"Cube\"\n",
    "    3. \"Playing_Cards\"\n",
    "    4. \"Screwdriver\"\n",
    "    5. \"Torch\"\n",
    "    which are redefined as binary later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_classes = 5\n",
    "merch_labels = [\"Cap\", \"Cube\", \"Playing_Cards\", \"Screwdriver\", \"Torch\"]\n",
    "merch_labels_numerical = [0, 1, 2, 3, 4]\n",
    "merch_data_paths = {}\n",
    "\n",
    "for i in range(len(merch_labels)):\n",
    "    merch_data_paths[merch_labels[i]] = os.getcwd() + \"\\\\MerchData\\\\\" + merch_labels[i]\n",
    "    \n",
    "#retrieves all files in folder\n",
    "X = []\n",
    "T = []\n",
    "for i in range(len(merch_labels)): #goes beyond to match labels\n",
    "    data_path = merch_data_paths[merch_labels[i-1]]\n",
    "#     merch_data_paths[\"Cap\"]\n",
    "    file_names = os.listdir(data_path)\n",
    "    for j in range(len(file_names)):\n",
    "        file_path = data_path + \"\\\\\" + file_names[j]\n",
    "        img = cv.resize(cv.imread(file_path), dsize=(32,32))\n",
    "        #img is resized to match the networks' input sizee\n",
    "        X.append(img)\n",
    "        T.append(i) #label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (np.asarray(X,dtype=np.float32)/255).astype('float16')\n",
    "T = np.asarray(T,dtype=np.float32)\n",
    "no_samples_train = 45\n",
    "\n",
    "indeces = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_test = indeces[:no_samples_train], indeces[no_samples_train:]\n",
    "#samples\n",
    "X_train2 = X[idx_train]\n",
    "X_test2 = X[idx_test]\n",
    "#targets\n",
    "t_train2 = keras.utils.np_utils.to_categorical(T[idx_train], no_classes)\n",
    "t_test2 = keras.utils.np_utils.to_categorical(T[idx_test], no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained CNN\n",
    "    trained on the CIFAR-10 dataset and freezing top layers\n",
    "    number of epochs was much higer than the suggested by the assingment\n",
    "    (I had access to a GPU with 12 GB of VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "if 'alexSnet' in globals():\n",
    "    del alexSnet\n",
    "\n",
    "weight_decay = 1e-6\n",
    "alexSnet = load_model('alexSnet\\\\alexSnet.h5')\n",
    "\n",
    "for i in range(len(alexSnet.layers)):\n",
    "    alexSnet.layers[i].trainable = False\n",
    "# for i in range(0,-7,-1):\n",
    "#     alexSnet.layers[i].trainable = True\n",
    "for i in range(6):\n",
    "    alexSnet.pop() #removes last two layers\n",
    "\n",
    "alexSnet.add(Dense(128,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "alexSnet.layers[-1].trainable = True\n",
    "alexSnet.add(Activation('relu'))\n",
    "alexSnet.add(BatchNormalization())\n",
    "alexSnet.add(Dropout(0.5))\n",
    "alexSnet.add(Dense(no_classes))\n",
    "alexSnet.layers[-1].trainable = True\n",
    "alexSnet.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "alexSnet.compile(loss='categorical_crossentropy',\\\n",
    "                 optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling and running the network(again!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_merch = 200\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "alexSnet.summary()\n",
    "# for i in range(len(VGG_NN.layers)):\n",
    "#     VGG_NN.layers[i].trainable = False\n",
    "cnn = alexSnet.fit_generator(datagen.flow(X_train2, t_train2,\\\n",
    "                                        batch_size=batch_size),\\\n",
    "                           steps_per_epoch=X_train2.shape[0] // batch_size,\\\n",
    "                           epochs=epochs_merch, validation_data=(X_test2, t_test2),\\\n",
    "                           callbacks=[reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check performance of transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_net.save_weights('transfer_cifar10alexSnet83.h5')\n",
    "\n",
    "# # del model  # deletes the existing model\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(cnn.history['acc'],'r')\n",
    "plt.plot(cnn.history['val_acc'],'g')\n",
    "plt.xticks(np.arange(0, epochs_merch+1, epochs_merch//10))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"transfer learning - train accuracy vs validation accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    " \n",
    "plt.figure(1)\n",
    "plt.plot(cnn.history['loss'],'r')\n",
    "plt.plot(cnn.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, epochs_merch+1, epochs_merch//10))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"transfer learning - train Loss vs validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "scores = alexSnet.evaluate(X_test2, t_test2, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alexSnet.save('alexSnet_transfer_merch.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training from scratch on the Merch dataset\n",
    "    the function is copied here again to prevent refeences to the same memory location for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=learning_rate, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "def alexsoaresnet2(no_outputs=10,weights_path=None):\n",
    "    weight_decay = 1e-6\n",
    "    model = Sequential()\n",
    "####################\n",
    "#     model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(48, (4, 4), strides=2, padding='valid', \\\n",
    "                     input_shape=(32,32,3), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=1))\n",
    "####################\n",
    "    model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(192, (3, 3), padding='valid', \\\n",
    "                     input_shape=(13,13,24), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=1))\n",
    "####################\n",
    "    model.add(keras.layers.ZeroPadding2D(padding=(1, 1)))\n",
    "    model.add(Conv2D(128, (3, 3), padding='valid', \\\n",
    "                     input_shape=(11,11,96), \\\n",
    "                     kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=1))\n",
    "####################\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "####################\n",
    "    model.add(Dense(no_outputs))\n",
    "    model.add(Activation('softmax'))    \n",
    "    \n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\\\n",
    "                  optimizer=sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    return learning_rate * (0.5 ** (epoch // lr_drop))\n",
    "\n",
    "reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleSnet_merch = alexsoaresnet2(no_outputs=5)\n",
    "epochs_merch = 200\n",
    "batch_size = 10\n",
    "\n",
    "aleSnet_merch.summary()\n",
    "# for i in range(len(VGG_NN.layers)):\n",
    "#     VGG_NN.layers[i].trainable = False\n",
    "aleSnet_merch_CNN = alexSnet.fit_generator(datagen.flow(X_train2, t_train2,\\\n",
    "                                        batch_size=batch_size),\\\n",
    "                           steps_per_epoch=X_train2.shape[0] // batch_size,\\\n",
    "                           epochs=epochs_merch, validation_data=(X_test2, t_test2),\\\n",
    "                           callbacks=[reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking performance of training form scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_net.save_weights('transfer_cifar10alexSnet83.h5')\n",
    "# # del model  # deletes the existing model\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(aleSnet_merch_CNN.history['acc'],'r')\n",
    "plt.plot(aleSnet_merch_CNN.history['val_acc'],'g')\n",
    "plt.xticks(np.arange(0, epochs_merch+1, epochs_merch//10))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.title(\"training from scratch: train accuracy vs validation accuracy\")\n",
    "plt.legend(['train','validation'])\n",
    " \n",
    "plt.figure(1)\n",
    "plt.plot(aleSnet_merch_CNN.history['loss'],'r')\n",
    "plt.plot(aleSnet_merch_CNN.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, epochs_merch+1, epochs_merch//10))\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"training from scratch: train Loss vs validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()\n",
    "scores = aleSnet_merch.evaluate(X_test2, t_test2, verbose=0)\n",
    "print(\"Test Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aleSnet_merch.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleSnet_merch.evaluate(X_test2, t_test2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
